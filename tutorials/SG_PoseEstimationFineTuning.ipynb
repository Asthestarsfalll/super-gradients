{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pose Estimation Fine Tuninig with Super Gradients\n",
    "\n",
    "In this tutorial notebook we demonstrate how to fine tune a pose estimation model using SuperGradients. It is recommended that you go over [Pose Estimation tutorial](https://docs.deci.ai/super-gradients/documentation/source/PoseEstimation.html) docs first to get familiar with terminology and concepts we use here.\n",
    "\n",
    "From this tutorial you will learn:\n",
    "* How to implement a custom dataset class for pose estimation task\n",
    "* How to instantiate a pre-trained pose estimation model and change number of joints it predicts to fit your dataset\n",
    "* How to fine-tune a pose estimation model using SuperGradients\n",
    "* How to visualize training progress and results in Tensorboard\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "The first thing we need is our dataset. For this tutorial we will be using [Animals Pose](https://sites.google.com/view/animal-pose/) dataset. It is a relatively small dataset of 6K+ instances of animals, each annotated with 20 keypoints.\n",
    "\n",
    "![](https://lh6.googleusercontent.com/hehW9yRzdcniQ2i1Ts65ceGERa70cBbaLlRixxu7HlUMHabt8HdgcxutG4vmVOas-U1h6g=w16383)\n",
    "\n",
    "### Download dataset from here\n",
    "\n",
    "https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x?usp=sharing\n",
    "\n",
    "You need to download the Animal Pose Dataset from Google Drive and update `ANIMALS_POSE_DATA_DIR` to point to this location:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ANIMALS_POSE_DATA_DIR = \"e:/animalpose\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:10.332075300Z",
     "start_time": "2023-06-05T11:08:10.178190700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset parsing\n",
    "\n",
    "Animals Pose dataset uses COCO-style annotation format.\n",
    "Unfortunately it's not 100% compatible with COCO parser from `pycocotools`.\n",
    "So we have to write out own parser. That is not a big issue, since the format is very simple.\n",
    "There are 3 main parts of the annotation file: \"images\", \"annotations\" and \"categories\".\n",
    "\n",
    "Here is an example of the annotation file:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"images\": {\n",
    "        \"1\": \"2007_000063.jpg\",\n",
    "        \"2\": \"2007_000175.jpg\",\n",
    "        \"6\": \"2007_000491.jpg\",\n",
    "        ...\n",
    "        \"4606\": \"sh97.jpg\",\n",
    "        \"4607\": \"sh98.jpeg\",\n",
    "        \"4608\": \"sh99.jpeg\"\n",
    "    },\n",
    "    \"annotations\": [\n",
    "        {\n",
    "            \"image_id\": 1,\n",
    "            \"bbox\": [123, 115, 379, 275],\n",
    "            \"keypoints\": [\n",
    "                [193, 216, 1],\n",
    "                [160, 217, 1],\n",
    "                ...,\n",
    "                [190, 145, 1],\n",
    "                [351, 238, 1]\n",
    "            ],\n",
    "            \"num_keypoints\": 20,\n",
    "            \"category_id\": 1\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\n",
    "            \"supercategory\": \"animal\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"dog\",\n",
    "            \"keypoints\": [\n",
    "                \"left_eye\",\n",
    "                \"right_eye\",\n",
    "                ...\n",
    "                \"throat\",\n",
    "                \"withers\",\n",
    "                \"tailbase\"\n",
    "            ],\n",
    "            \"skeleton\": [\n",
    "                [0, 1],\n",
    "                [0, 2],\n",
    "                ...\n",
    "                [11, 15],\n",
    "                [12, 16]\n",
    "            ]\n",
    "        },\n",
    "```\n",
    "\n",
    "To train pose estimation model using Super Gradients we need to implement a custom dataset class that will parse this format and return images and targets for the model.\n",
    "Fortunately, Super Gradients provides a base class for pose estimation datasets that handles most of the boilerplate code for us.\n",
    "\n",
    "We need to implement a dataset parsing method that will return annotations in a format that Super Gradients expects.\n",
    "The dataset class is expected to have the following method signature:\n",
    "\n",
    "```python\n",
    "class AnimalsPoseDataset:\n",
    "    def __getitem__(self, index):\n",
    "        ...\n",
    "        return image, targets, {\"gt_joints\": gt_joints, \"gt_bboxes\": gt_bboxes, \"gt_iscrowd\": gt_iscrowd, \"gt_areas\": gt_areas}\n",
    "```\n",
    "\n",
    "The `__getitem__` method is expected to return a tuple of 3 elements `image`, `targets` and `extras`:\n",
    "\n",
    "* `image` - torch tensor of [C,H,W] shape that represents an input image to the model.\n",
    "* `targets` - model-specific targets to train the model itself. Fortunately SG will take care of generating these targets for us. Our goal is to provide the keypoints of [Num Instances, Num Joints, 3] shape.\n",
    "* `extras` - Additional information with poses for metric computation. Must be a dictionary with following keys:\n",
    "    * gt_joints - Array of keypoints for all poses in the image. Numpy array of [Num Instances, Num Joints, 3] shape\n",
    "    * gt_bboxes - Array of bounding boxes (XYWH) for each pose in the image. Numpy array of [Num Instances, 4] shape\n",
    "    * gt_iscrowd - Array of iscrowd flags for each pose in the image. Numpy array of [Num Instances] shape\n",
    "    * gt_areas - Array of areas for each skeleton in the image. Numpy array of [Num Instances] shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into C:\\Users\\blood\\sg_logs\\console.log\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Mapping, Any, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "from super_gradients.common.decorators.factory_decorator import resolve_param\n",
    "from super_gradients.common.factories.target_generator_factory import TargetGeneratorsFactory\n",
    "from super_gradients.common.factories.transforms_factory import TransformsFactory\n",
    "from super_gradients.training.datasets.pose_estimation_datasets import BaseKeypointsDataset\n",
    "from super_gradients.training.transforms.keypoint_transforms import KeypointTransform\n",
    "\n",
    "\n",
    "class AnimalPoseKeypointsDataset(BaseKeypointsDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for training pose estimation models on Animal Pose dataset.\n",
    "    User should pass a target generator class that is model-specific and generates the targets for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    @resolve_param(\"transforms\", TransformsFactory())\n",
    "    @resolve_param(\"target_generator\", TargetGeneratorsFactory())\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        images_dir: str,\n",
    "        json_file: str,\n",
    "        include_empty_samples: bool,\n",
    "        target_generator,\n",
    "        transforms: List[KeypointTransform],\n",
    "        min_instance_area: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data_dir: Root directory of the COCO dataset\n",
    "        :param images_dir: path suffix to the images directory inside the data_dir\n",
    "        :param json_file: path suffix to the json file inside the data_dir\n",
    "        :param include_empty_samples: Not used, but exists for compatibility with COCO dataset config.\n",
    "        :param target_generator: Target generator that will be used to generate the targets for the model.\n",
    "            See DEKRTargetsGenerator for an example.\n",
    "        :param transforms: Transforms to be applied to the image & keypoints\n",
    "        :param min_instance_area: Minimum area of an instance to be included in the dataset\n",
    "        \"\"\"\n",
    "        super().__init__(transforms=transforms, target_generator=target_generator, min_instance_area=min_instance_area)\n",
    "\n",
    "        with open(os.path.join(data_dir, json_file), \"r\") as f:\n",
    "            json_annotations = json.load(f)\n",
    "\n",
    "        self.joints = json_annotations[\"categories\"][0][\"keypoints\"]\n",
    "        self.num_joints = len(self.joints)\n",
    "\n",
    "        images_and_ids = [(image_id, os.path.join(data_dir, images_dir, image_path)) for image_id, image_path in json_annotations[\"images\"].items()]\n",
    "        self.image_ids, self.image_files = zip(*images_and_ids)\n",
    "\n",
    "        self.annotations = []\n",
    "\n",
    "        for image_id in self.image_ids:\n",
    "            keypoints_per_image = []\n",
    "            bboxes_per_image = []\n",
    "\n",
    "            image_annotations = [ann for ann in json_annotations[\"annotations\"] if str(ann[\"image_id\"]) == str(image_id)]\n",
    "            for ann in image_annotations:\n",
    "                keypoints = np.array(ann[\"keypoints\"]).reshape(self.num_joints, 3)\n",
    "                x1, y1, x2, y2 = ann[\"bbox\"]\n",
    "\n",
    "                bbox_xywh = np.array([x1, y1, x2 - x1, y2 - y1])\n",
    "                keypoints_per_image.append(keypoints)\n",
    "                bboxes_per_image.append(bbox_xywh)\n",
    "\n",
    "            keypoints_per_image = np.array(keypoints_per_image, dtype=np.float32).reshape(-1, self.num_joints, 3)\n",
    "            bboxes_per_image = np.array(bboxes_per_image, dtype=np.float32).reshape(-1, 4)\n",
    "            annotation = keypoints_per_image, bboxes_per_image\n",
    "            self.annotations.append(annotation)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def load_sample(self, index):\n",
    "        file_path = self.image_files[index]\n",
    "        gt_joints, gt_bboxes = self.annotations[index]  # boxes in xywh format\n",
    "\n",
    "        gt_areas = np.array([box[2] * box[3] for box in gt_bboxes], dtype=np.float32)\n",
    "        gt_iscrowd = np.array([0] * len(gt_joints), dtype=bool)\n",
    "\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "        mask = np.ones(image.shape[:2], dtype=np.float32)\n",
    "\n",
    "        return image, mask, gt_joints, gt_areas, gt_bboxes, gt_iscrowd\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Any, Mapping[str, Any]]:\n",
    "        img, mask, gt_joints, gt_areas, gt_bboxes, gt_iscrowd = self.load_sample(index)\n",
    "        img, mask, gt_joints, gt_areas, gt_bboxes = self.transforms(img, mask, gt_joints, areas=gt_areas, bboxes=gt_bboxes)\n",
    "\n",
    "        image_shape = img.size(1), img.size(2)\n",
    "        gt_joints, gt_areas, gt_bboxes, gt_iscrowd = self.filter_joints(image_shape, gt_joints, gt_areas, gt_bboxes, gt_iscrowd)\n",
    "\n",
    "        targets = self.target_generator(img, gt_joints, mask)\n",
    "        return img, targets, {\"gt_joints\": gt_joints, \"gt_bboxes\": gt_bboxes, \"gt_iscrowd\": gt_iscrowd, \"gt_areas\": gt_areas}\n",
    "\n",
    "    def filter_joints(\n",
    "        self,\n",
    "        image_shape,\n",
    "        joints: np.ndarray,\n",
    "        areas: np.ndarray,\n",
    "        bboxes: np.ndarray,\n",
    "        is_crowd: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter instances that are either too small or do not have visible keypoints.\n",
    "\n",
    "        :param image: Image if [H,W,C] shape. Used to infer image boundaries\n",
    "        :param joints: Array of shape [Num Instances, Num Joints, 3]\n",
    "        :param areas: Array of shape [Num Instances] with area of each instance.\n",
    "                      Instance area comes from segmentation mask from COCO annotation file.\n",
    "        :param bboxes: Array of shape [Num Instances, 4] for bounding boxes in XYWH format.\n",
    "                       Bounding boxes comes from segmentation mask from COCO annotation file.\n",
    "        :param: is_crowd: Array of shape [Num Instances] indicating whether an instance is a crowd target.\n",
    "        :return: [New Num Instances, Num Joints, 3], New Num Instances <= Num Instances\n",
    "        \"\"\"\n",
    "\n",
    "        # Update visibility of joints for those that are outside the image\n",
    "        outside_image_mask = (joints[:, :, 0] < 0) | (joints[:, :, 1] < 0) | (joints[:, :, 0] >= image_shape[1]) | (joints[:, :, 1] >= image_shape[0])\n",
    "        joints[outside_image_mask, 2] = 0\n",
    "\n",
    "        # Filter instances with all invisible keypoints\n",
    "        instances_with_visible_joints = np.count_nonzero(joints[:, :, 2], axis=-1) > 0\n",
    "        instances_with_good_area = areas > self.min_instance_area\n",
    "\n",
    "        keep_mask = instances_with_visible_joints & instances_with_good_area\n",
    "\n",
    "        joints = joints[keep_mask]\n",
    "        areas = areas[keep_mask]\n",
    "        bboxes = bboxes[keep_mask]\n",
    "        is_crowd = is_crowd[keep_mask]\n",
    "\n",
    "        return joints, areas, bboxes, is_crowd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:18.059324400Z",
     "start_time": "2023-06-05T11:08:10.206196400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's recap what are the responsibilities of `AnimalPoseKeypointsDataset` class:\n",
    "\n",
    "- load annotations from JSON file in COCO format\n",
    "- Unpack annotations into keypoints and bounding boxes\n",
    "- Implements a `__getitem__` method for retrieving individual image with corresponding annotations\n",
    "- Apply preprocessing and data augmentation transforms to the image and annotations\n",
    "- Encodes a \"human friendly\" joints representation (Array of `[Num Instances, Num Joints, 3]` shape) into a format suitable for training Yolo-NAS (Tuple of two tensors `[NumJoints + 1, H/4, W/4]`, `[NumJoints*2, H/4, W/4]`).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, when we have the dataset class we can instantiate DataLoaders.\n",
    "Since our fine-tuning tasks is really similar to COCO human pose estimation task we can take COCO dataset configs as base ones and use transforms & target generator from these configs:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from super_gradients.training.dataloaders import get_data_loader\n",
    "\n",
    "def animalpose_pose_train(dataset_params: Dict = None, dataloader_params: Dict = None) -> DataLoader:\n",
    "    return get_data_loader(\n",
    "        config_name=\"coco_pose_estimation_dekr_dataset_params\",\n",
    "        dataset_cls=AnimalPoseKeypointsDataset,\n",
    "        train=True,\n",
    "        dataset_params=dataset_params,\n",
    "        dataloader_params=dataloader_params,\n",
    "    )\n",
    "\n",
    "\n",
    "def animalpose_pose_val(dataset_params: Dict = None, dataloader_params: Dict = None) -> DataLoader:\n",
    "    return get_data_loader(\n",
    "        config_name=\"coco_pose_estimation_dekr_dataset_params\",\n",
    "        dataset_cls=AnimalPoseKeypointsDataset,\n",
    "        train=False,\n",
    "        dataset_params=dataset_params,\n",
    "        dataloader_params=dataloader_params,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:18.074328300Z",
     "start_time": "2023-06-05T11:08:18.060326500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are almost ready to instantiate our data loaders. There is only one important nuance that we have to cover. Let's instantiate our train dataset and inspect the transformations that we apply to our samples:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[KeypointsLongestMaxSize(max_height=640, max_width=640, interpolation=1, prob=1.0),\n KeypointsPadIfNeeded(min_height=640, min_width=640, image_pad_value=(127, 127, 127), mask_pad_value=1),\n KeypointsRandomHorizontalFlip(flip_index=[0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15], prob=0.5),\n KeypointsRandomAffineTransform(max_rotation=30, min_scale=0.5, max_scale=2, max_translate=0.2, image_pad_value=(127, 127, 127), mask_pad_value=1, prob=0.75),\n KeypointsImageToTensor(),\n KeypointsImageNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "train_data = animalpose_pose_train(\n",
    "    dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR, images_dir=\"images\", json_file=\"keypoints.json\"),\n",
    "    dataloader_params=dict(num_workers=0, batch_size=8)\n",
    ")\n",
    "train_data.dataset.transforms.transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:28.946373600Z",
     "start_time": "2023-06-05T11:08:18.078325100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training we apply several augmentations to our samples. There is one, however that needs out special attention, which is KeypointsRandomHorizontalFlip.\n",
    "When working with object detection and semantic segmentation we can safely skip the fact that left and right in flipped image changing sides.\n",
    "However, in pose estimation of objects that possess vertical symmetry, like animals or humans, we have to take this into account.\n",
    "So when we flip the image we also have to swap left and right keypoints. In order to do this correctly KeypointsRandomHorizontalFlip transform must know the rearrange indices for keypoints.\n",
    "\n",
    "If we look at the JSON data of your annotation file, we can see that the order of keypoints is the following:\n",
    "```\n",
    "# 0 \"left_eye\" -> \"right_eye\": 1\n",
    "# 1 \"right_eye\" -> \"left_eye\": 0\n",
    "# 2 \"nose\" -> \"nose\": 2\n",
    "# 3 \"left_ear\" -> \"right_ear\": 4\n",
    "# 4 \"right_ear\" -> \"left_ear\": 3\n",
    "# 5 \"left_front_elbow\" -> \"right_front_elbow\": 6\n",
    "# 6 \"right_front_elbow\" -> \"left_front_elbow\": 5\n",
    "# 7 \"left_back_elbow\" -> \"right_back_elbow\": 8\n",
    "# 8 \"right_back_elbow\" -> \"left_back_elbow\": 7\n",
    "# 9 \"left_front_knee\" -> \"right_front_knee\": 10\n",
    "# 10 \"right_front_knee\" -> \"left_front_knee\": 9\n",
    "# 11 \"left_back_knee\" -> \"right_back_knee\": 12\n",
    "# 12 \"right_back_knee\" -> \"left_back_knee\": 11\n",
    "# 13 \"left_front_paw\" -> \"right_front_paw\": 14\n",
    "# 14 \"right_front_paw\" -> \"left_front_paw\": 13\n",
    "# 15 \"left_back_paw\" -> \"right_back_paw\": 16\n",
    "# 16 \"right_back_paw\" -> \"left_back_paw\": 15\n",
    "# 17 \"throat\" -> \"throat\": 17\n",
    "# 18 \"withers\" -> \"withers\": 18\n",
    "# 10 \"tailbase\" -> \"tailbase\": 19\n",
    "```\n",
    "\n",
    "So our array of indexes will look like this:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ANIMALS_POSE_FLIP_INDEXES = [1,0,2,4,3,6,5,8,7,10,9,12,11,14,13,16,15,17,18,19]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:28.991178700Z",
     "start_time": "2023-06-05T11:08:28.950374800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For training data loader we have to pass the transforms explicitly in order to override the default ones with the right flip transform:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from super_gradients.training.transforms.keypoint_transforms import KeypointsLongestMaxSize, KeypointsPadIfNeeded, \\\n",
    "    KeypointsRandomHorizontalFlip, KeypointsRandomAffineTransform, KeypointsImageNormalize, KeypointsImageToTensor\n",
    "\n",
    "train_transforms = [\n",
    "    KeypointsLongestMaxSize(640, 640),\n",
    "    KeypointsPadIfNeeded(640, 640, image_pad_value=(127,127,127), mask_pad_value=1),\n",
    "    KeypointsRandomHorizontalFlip(ANIMALS_POSE_FLIP_INDEXES),\n",
    "    KeypointsRandomAffineTransform(max_rotation=30, min_scale=0.75, max_scale=1.25, max_translate=0.2, image_pad_value=(127,127,127), mask_pad_value=1, prob=0.5),\n",
    "    KeypointsImageToTensor(),\n",
    "    KeypointsImageNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "]\n",
    "\n",
    "train_data = animalpose_pose_train(\n",
    "    dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR, images_dir=\"images\", json_file=\"keypoints.json\", transforms=train_transforms),\n",
    "    dataloader_params=dict(num_workers=0, batch_size=16)\n",
    ")\n",
    "\n",
    "val_data = animalpose_pose_val(\n",
    "    dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR, images_dir=\"images\", json_file=\"keypoints.json\"),\n",
    "    dataloader_params=dict(num_workers=0, batch_size=16)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:51.564385300Z",
     "start_time": "2023-06-05T11:08:28.969004600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "KeypointsCompose(\n    KeypointsLongestMaxSize(max_height=640, max_width=640, interpolation=1, prob=1.0)\n    KeypointsPadIfNeeded(min_height=640, min_width=640, image_pad_value=(127, 127, 127), mask_pad_value=1)\n    KeypointsRandomHorizontalFlip(flip_index=[1, 0, 2, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15, 17, 18, 19], prob=0.5)\n    KeypointsRandomAffineTransform(max_rotation=30, min_scale=0.75, max_scale=1.25, max_translate=0.2, image_pad_value=(127, 127, 127), mask_pad_value=1, prob=0.5)\n    KeypointsImageToTensor()\n    KeypointsImageNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset.transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:51.580278800Z",
     "start_time": "2023-06-05T11:08:51.564385300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "KeypointsCompose(\n    KeypointsLongestMaxSize(max_height=640, max_width=640, interpolation=1, prob=1.0)\n    KeypointsPadIfNeeded(min_height=640, min_width=640, image_pad_value=(127, 127, 127), mask_pad_value=1)\n    KeypointsImageToTensor()\n    KeypointsImageNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.dataset.transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:08:51.670651Z",
     "start_time": "2023-06-05T11:08:51.582278400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Instantiating the model for fine-tuning\n",
    "\n",
    "In this tutorial we will use pre-trained YOLO-NAS for pose estimation model. It is trained on COCO dataset for human pose estimation.\n",
    "We will fine-tune it on our dataset. In order to do this we have to replace the last layer of the model with the new one that will predict 20 keypoints instead of 17.\n",
    "\n",
    "Super Gradients provide convenient way to instantiate pre-trained models and override the number of output classes: `models.get(Models.YOLO_NAS_POSE_L, num_classes=20, pretrained_weights=\"coco\")`.\n",
    "\n",
    "What is happening under the hood in this call is the following:\n",
    "1. `models.get(Models.YOLO_NAS_POSE_L, ...)` - this call instantiates the model Models.YOLO_NAS_POSE_L with default parameters.\n",
    "2. `models.get(..., num_classes=20, ...)` - this parameter sets the target number of output classes the model should have. Here it is set to 20 as this is the target number of joints in our dataset.\n",
    "3. `models.get(..., pretrained_weights=\"coco\")` - this parameter specify what pretrained weights should be loaded into the model. Here we specify that we want to load weights trained on COCO dataset for human pose estimation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "yolo_nas_pose_l = models.get(Models.YOLO_NAS_POSE_L, num_classes=20, pretrained_weights=\"coco\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:09:17.261956600Z",
     "start_time": "2023-06-05T11:08:51.611961500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Training hyperparameters\n",
    "\n",
    "Training YOLO-NAS requires passing a specific loss, which we set up in our training hyperparameters. YOLO-NAS Pose uses same target encoding as DEKR architecture, however for training objective we don't use MSE loss as in DEKR, but QFL loss instead. Therefore, we have to pass the right loss function to the training hyperparameters:\n",
    "\n",
    " ```yaml\n",
    "    \"loss\": \"dekr_loss\",\n",
    "    \"criterion_params\": {\n",
    "        \"heatmap_loss\": \"qfl\",\n",
    "        \"heatmap_loss_factor\": 1.0,\n",
    "        \"offset_loss_factor\": 0.1,\n",
    "    }\n",
    "```\n",
    "\n",
    "We do not use warmup for this example, but you can see how to set it up in the commented out section below:\n",
    "\n",
    "```yaml\n",
    "    \"warmup_mode\": \"linear_epoch_step\",\n",
    "    \"warmup_initial_lr\": 1e-3,\n",
    "    \"lr_warmup_epochs\": 0,\n",
    "```\n",
    "\n",
    "To compute the target metric we add `PoseEstimationMetrics` object to `valid_metrics_list`:\n",
    "\n",
    "```yaml\n",
    "    \"valid_metrics_list\": [\n",
    "        PoseEstimationMetrics(\n",
    "            num_joints=20,\n",
    "            oks_sigmas=None,\n",
    "            max_objects_per_image=30,\n",
    "            post_prediction_callback=yolo_nas_pose_l.get_post_prediction_callback(conf=0.05, iou=0.05),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": 'AP',\n",
    "```\n",
    "\n",
    "And last but not least, let's add visualization callbacks to see how our training is going. The results will be available in Tensorboard logs:\n",
    "\n",
    "```yaml\n",
    "    \"phase_callbacks\": [\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.TRAIN_BATCH_END, prefix=\"train_\"),\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.VALIDATION_BATCH_END, prefix=\"val\"),\n",
    "    ]\n",
    "```\n",
    "\n",
    "Final training params config should look like this:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from super_gradients.training.utils import DEKRVisualizationCallback\n",
    "from super_gradients.training.metrics import PoseEstimationMetrics\n",
    "from super_gradients.training.utils.callbacks.callbacks import Phase\n",
    "\n",
    "train_params = {\n",
    "    \"average_best_models\": True,\n",
    "    \"warmup_mode\": \"linear_epoch_step\",\n",
    "    \"warmup_initial_lr\": 1e-3,\n",
    "    \"lr_warmup_epochs\": 0,\n",
    "    \"initial_lr\": 1e-3,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.1,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"optimizer_params\": {\"weight_decay\": 1e-6},\n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"ema\": False,\n",
    "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},\n",
    "    # ONLY TRAINING FOR 10 EPOCHS FOR THIS EXAMPLE NOTEBOOK\n",
    "    \"max_epochs\": 10,\n",
    "    \"mixed_precision\": True,\n",
    "    \"loss\": \"dekr_loss\",\n",
    "    \"criterion_params\": {\n",
    "        \"heatmap_loss\": \"qfl\",\n",
    "        \"heatmap_loss_factor\": 1.0,\n",
    "        \"offset_loss_factor\": 0.1,\n",
    "    },\n",
    "    \"valid_metrics_list\": [\n",
    "        PoseEstimationMetrics(\n",
    "            num_joints=20,\n",
    "            oks_sigmas=None,\n",
    "            max_objects_per_image=30,\n",
    "            post_prediction_callback=yolo_nas_pose_l.get_post_prediction_callback(conf=0.05, iou=0.05),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": 'AP',\n",
    "    \"phase_callbacks\": [\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.TRAIN_BATCH_END, prefix=\"train_\"),\n",
    "        DEKRVisualizationCallback(mean=[ 0.485, 0.456, 0.406 ], std=[ 0.229, 0.224, 0.225 ], apply_sigmoid=True, phase=Phase.VALIDATION_BATCH_END, prefix=\"val\"),\n",
    "    ]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:36.323186400Z",
     "start_time": "2023-06-05T11:25:36.289757700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from super_gradients.training import Trainer\n",
    "\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:40.848162Z",
     "start_time": "2023-06-05T11:25:40.815163400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-c5dd049dffa8ba5e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-c5dd049dffa8ba5e\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir checkpoints"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:41.409106300Z",
     "start_time": "2023-06-05T11:25:41.366238300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "trainer = Trainer(experiment_name='animal_pose_fine_tuning', ckpt_root_dir=CHECKPOINT_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-05T11:25:49.075229300Z",
     "start_time": "2023-06-05T11:25:49.040542300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is now moved to checkpoints\\animal_pose_fine_tuning/console_июнь05_14_25_49.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0: 100%|██████████| 288/288 [07:26<00:00,  1.55s/it, DEKRLoss/heatmap=0.00176, DEKRLoss/offset=0.00362, DEKRLoss/total=0.00538, gpu_mem=17.1]\n",
      "Validation epoch 0: 100%|██████████| 288/288 [06:15<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 0\n",
      "├── Training\n",
      "│   ├── Dekrloss/heatmap = 0.0018\n",
      "│   ├── Dekrloss/offset = 0.0036\n",
      "│   └── Dekrloss/total = 0.0054\n",
      "└── Validation\n",
      "    ├── Ap = 0.0042\n",
      "    ├── Ar = 0.1098\n",
      "    ├── Dekrloss/heatmap = 0.0006\n",
      "    ├── Dekrloss/offset = 0.0032\n",
      "    └── Dekrloss/total = 0.0038\n",
      "\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1:  36%|███▋      | 105/288 [02:44<04:33,  1.49s/it, DEKRLoss/heatmap=0.000546, DEKRLoss/offset=0.00306, DEKRLoss/total=0.0036, gpu_mem=17.3] "
     ]
    }
   ],
   "source": [
    "trainer.train(model=yolo_nas_pose_l,\n",
    "              training_params=train_params,\n",
    "              train_loader=train_data,\n",
    "              valid_loader=val_data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-05T11:25:49.340357100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
