{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Offline evaluation of ONNX Pose Estimation models\n",
    "\n",
    "This example shows how to evaluate **exported** pose estimation models using pycocotools and onnxrunime package.\n",
    "Although we provide a ready-to-use metric class to compute average precision (AP) and average recall (AR) scores, the \n",
    "evaluation protocol during validation is slightly different from what pycocotools suggests for academic evaluation.\n",
    "\n",
    "In particular:\n",
    "\n",
    "## SG\n",
    "\n",
    "* In SG, during training/validation, we resize all images to a fixed size (Default is 640x640) using aspect-ratio preserving resize of the longest size + padding. \n",
    "* Our metric evaluate AP/AR in the resolution of the resized & padded images, **not in the resolution of original image**. \n",
    "\n",
    "\n",
    "## COCOEval\n",
    "\n",
    "* In COCOEval all images are not resized and pose predictions are evaluated in the resolution of original image \n",
    "\n",
    "Because of this discrepancy, metrics reported by `PoseEstimationMetrics` class is usually a bit lower (Usually by ~1AP) than the ones \n",
    "you would get from the same model if computed with COCOEval. \n",
    "\n",
    "For this reason we provide this example to show how you can compute metrics using COCOEval for pose estimation models that are available in SuperGradients."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36fcd2e6daa861c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate the model for evaluation\n",
    "\n",
    "First, let's instantiate the model we are going to evaluate. \n",
    "You can use either pretrained models or provide a checkpoint path to your own trained checkpoint.\n",
    "\n",
    "```python\n",
    "model = models.get(Models.YOLO_NAS_POSE_N, pretrained_weights=\"coco_pose\")\n",
    "```\n",
    "In this example we will be using pretrained weights for simplicity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33367b7bd09fccbb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into C:\\Users\\ekhve\\sg_logs\\console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1025 16:20:31.074215 21720 redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "W1025 16:20:35.444142 21720 __init__.py:76] scikit-learn version 1.3.0 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "W1025 16:20:35.449141 21720 __init__.py:36] Torch version 2.0.1+cu118 has not been tested with coremltools. You may run into unexpected errors. Torch 2.0.0 is the most recent version that has been tested.\n",
      "W1025 16:20:35.648410 21720 load.py:35] Fail to import BlobReader from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n",
      "W1025 16:20:35.653998 21720 load.py:46] Fail to import BlobWriter from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekhve\\.conda\\envs\\sg\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5589: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\nModel exported successfully to yolo_nas_pose_n.onnx\nModel expects input image of shape [1, 3, 640, 640]\nInput image dtype is torch.uint8\n\nExported model already contains preprocessing (normalization) step, so you don't need to do it manually.\nPreprocessing steps to be applied to input image are:\nSequential(\n  (0): CastTensorTo(dtype=torch.float32)\n  (1): ChannelSelect(channels_indexes=tensor([2, 1, 0], device='cuda:0'))\n  (2): ApplyMeanStd(mean=[0.], scale=[255.])\n)\n\n\nExported model contains postprocessing (NMS) step with the following parameters:\n    num_pre_nms_predictions=1000\n    max_predictions_per_image=20\n    nms_threshold=0.7\n    confidence_threshold=0.01\n    output_predictions_format=flat\n\n\nExported model is in ONNX format and can be used with ONNXRuntime\nTo run inference with ONNXRuntime, please use the following code snippet:\n\n    import onnxruntime\n    import numpy as np\n    session = onnxruntime.InferenceSession(\"yolo_nas_pose_n.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n    inputs = [o.name for o in session.get_inputs()]\n    outputs = [o.name for o in session.get_outputs()]\n\n    example_input_image = np.zeros((1, 3, 640, 640)).astype(np.uint8)\n    predictions = session.run(outputs, {inputs[0]: example_input_image})\n\nExported model can also be used with TensorRT\nTo run inference with TensorRT, please see TensorRT deployment documentation\nYou can benchmark the model using the following code snippet:\n\n    trtexec --onnx=yolo_nas_pose_n.onnx --fp16 --avgRuns=100 --duration=15\n\n\nExported model has predictions in flat format:\n\n# flat_predictions is a 2D array of [N,K] shape\n# Each row represents (image_index, x_min, y_min, x_max, y_max, confidence, joints...)\n# Please note all values are floats, so you have to convert them to integers if needed\n\n[flat_predictions] = predictions\npred_bboxes = flat_predictions[:, 1:5]\npred_scores = flat_predictions[:, 5]\npred_joints = flat_predictions[:, 6:].reshape((len(pred_bboxes), -1, 3))\nfor i in range(len(pred_bboxes)):\n    confidence = pred_scores[i]\n    x_min, y_min, x_max, y_max = pred_bboxes[i]\n    print(f\"Detected pose with confidence={{confidence}}, x_min={{x_min}}, y_min={{y_min}}, x_max={{x_max}}, y_max={{y_max}}\")\n    for joint_index, (x, y, confidence) in enumerate(pred_joints[i]):\")\n        print(f\"Joint {{joint_index}} has coordinates x={{x}}, y={{y}}, confidence={{confidence}}\")\n"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from super_gradients.conversion import DetectionOutputFormatMode\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "model = models.get(Models.YOLO_NAS_POSE_N, pretrained_weights=\"coco_pose\").cuda()\n",
    "\n",
    "result = model.export(\n",
    "    \"yolo_nas_pose_n.onnx\",\n",
    "    confidence_threshold=0.01,\n",
    "    max_predictions_per_image=20, nms_threshold=0.7, output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT\n",
    ")\n",
    "result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:47.416927200Z",
     "start_time": "2023-10-25T13:20:26.135059500Z"
    }
   },
   "id": "36c9acfa4c5057c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare COCO validation data\n",
    "\n",
    "Next, we obtain list of images in COCO2017 validation set and load their annotations.\n",
    "You may want to either set the COCO_ROOT_DIR environment variable where COCO2017 data is located on your machine or edit the default path directylu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372d6ce9605c68f9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['annotations', 'images']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "COCO_DATA_DIR = os.environ.get(\"COCO_ROOT_DIR\", \"g:/coco2017\")\n",
    "os.listdir(COCO_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:47.433944900Z",
     "start_time": "2023-10-25T13:20:47.416927200Z"
    }
   },
   "id": "91e2ee3f26130e1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once data is set we can load it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a8be6a43b014149"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:47.513128800Z",
     "start_time": "2023-10-25T13:20:47.437970Z"
    }
   },
   "id": "ca413a0cfd65d3d9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "images_path = os.path.join(COCO_DATA_DIR, \"images/val2017\")\n",
    "image_files = [os.path.join(images_path, x) for x in os.listdir(images_path)]\n",
    "\n",
    "gt_annotations_path = os.path.join(COCO_DATA_DIR, \"annotations/person_keypoints_val2017.json\")\n",
    "gt = COCO(gt_annotations_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:48.125605100Z",
     "start_time": "2023-10-25T13:20:47.453984800Z"
    }
   },
   "id": "31ef5c6c808fc5c2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(['onnx::Cast_0'], ['graph2_flat_predictions'])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"yolo_nas_pose_n.onnx\",\n",
    "                                       providers=[\n",
    "                                           \"CUDAExecutionProvider\", \n",
    "                                           \"CPUExecutionProvider\"\n",
    "                                        ])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "inputs, outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:48.264194400Z",
     "start_time": "2023-10-25T13:20:48.124209400Z"
    }
   },
   "id": "7d20871ce397f6cb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekhve\\.conda\\envs\\sg\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:54: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "ComposeProcessingMetadata(metadata_lst=[RescaleMetadata(original_shape=(500, 375), scale_factor_h=1.28, scale_factor_w=1.28), DetectionPadToSizeMetadata(padding_coordinates=PaddingCoordinates(top=0, bottom=0, left=0, right=160)), None])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from super_gradients.training.utils.predict import PoseEstimationPrediction\n",
    "from super_gradients.module_interfaces import PoseEstimationPredictions\n",
    "import torch\n",
    "from super_gradients.training.processing import ComposeProcessing\n",
    "from super_gradients.training.processing.processing import KeypointsLongestMaxSizeRescale\n",
    "from super_gradients.training.processing.processing import KeypointsBottomRightPadding\n",
    "from super_gradients.training.processing import ImagePermute\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_processor = ComposeProcessing(\n",
    "    [\n",
    "        KeypointsLongestMaxSizeRescale(output_shape=(640, 640)),\n",
    "        KeypointsBottomRightPadding(output_shape=(640, 640), pad_value=127),\n",
    "        ImagePermute(permutation=(2, 0, 1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_original = cv2.imread(image_files[3])[..., ::-1]  # BGR -> RGB\n",
    "\n",
    "# Resize image to 640x640\n",
    "image_resized, metadata = image_processor.preprocess_image(image_original)\n",
    "model_input = np.expand_dims(image_resized, 0)  # [1,3,640,640]\n",
    "[flat_predictions] = session.run(outputs, {inputs[0]: model_input})\n",
    "\n",
    "pred_bboxes = flat_predictions[:, 1:5]\n",
    "pred_scores = flat_predictions[:, 5]\n",
    "pred_joints = flat_predictions[:, 6:].reshape(len(pred_bboxes), -1, 3)\n",
    "\n",
    "p = PoseEstimationPrediction(\n",
    "    poses=pred_joints,\n",
    "    scores=pred_scores,\n",
    "    bboxes_xyxy=pred_bboxes,\n",
    "    edge_links=np.zeros((0,2)),\n",
    "    edge_colors=np.zeros((0,3)),\n",
    "    keypoint_colors=np.random.randint(0, 255, (pred_joints.shape[1], 3), dtype=np.uint8),\n",
    "    image_shape=(model_input.shape[2], model_input.shape[3]),\n",
    ")\n",
    "metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:48.466221700Z",
     "start_time": "2023-10-25T13:20:48.273285700Z"
    }
   },
   "id": "fb69db3d22a04e41"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "PoseEstimationPrediction(poses=array([[[ 9.33892746e+01,  3.71910980e+02,  2.40387648e-01],\n        [ 8.84057465e+01,  3.70263672e+02,  2.12954700e-01],\n        [ 9.35746536e+01,  3.70570801e+02,  2.05087841e-01],\n        ...,\n        [ 9.88854828e+01,  4.10569458e+02,  2.41989732e-01],\n        [ 8.94517212e+01,  4.17730042e+02,  1.46408051e-01],\n        [ 1.00782990e+02,  4.17586487e+02,  1.42147750e-01]],\n\n       [[ 1.18740906e+02,  3.46397156e+02,  4.15875465e-01],\n        [ 1.18329468e+02,  3.43987854e+02,  2.89114654e-01],\n        [ 1.17013855e+02,  3.44044250e+02,  4.05241132e-01],\n        ...,\n        [ 1.19035332e+02,  4.14153992e+02,  2.14025617e-01],\n        [ 1.07612869e+02,  4.25690491e+02,  1.13747567e-01],\n        [ 1.17378853e+02,  4.25458984e+02,  1.11210644e-01]],\n\n       [[ 4.15635132e+02,  5.14472900e+02,  2.96225727e-01],\n        [ 4.18026947e+02,  5.10613037e+02,  3.20348173e-01],\n        [ 4.17845551e+02,  5.10465118e+02,  1.34563357e-01],\n        ...,\n        [ 4.33143372e+02,  5.94633423e+02,  1.79093242e-01],\n        [ 4.14871338e+02,  6.24720093e+02,  1.60950810e-01],\n        [ 4.29955627e+02,  6.24496460e+02,  1.30747378e-01]],\n\n       ...,\n\n       [[ 6.31943893e+00,  3.19803284e+02,  2.33334541e-01],\n        [ 4.56894493e+00,  3.16665405e+02,  1.46367788e-01],\n        [ 5.90148020e+00,  3.16940796e+02,  2.34512687e-01],\n        ...,\n        [-1.96627617e-01,  3.57455536e+02,  2.33991444e-01],\n        [ 2.17940807e+00,  3.89801941e+02,  1.92468643e-01],\n        [-6.72112346e-01,  3.90139740e+02,  2.05798358e-01]],\n\n       [[ 6.25247154e+01,  3.19454865e+02,  4.68091160e-01],\n        [ 6.38847351e+01,  3.16827484e+02,  3.08891773e-01],\n        [ 5.94246864e+01,  3.16975342e+02,  4.79910582e-01],\n        ...,\n        [ 5.21140785e+01,  3.59242889e+02,  1.30137980e-01],\n        [ 6.29594574e+01,  3.67403870e+02,  9.61355567e-02],\n        [ 5.35664597e+01,  3.68449768e+02,  9.76549983e-02]],\n\n       [[ 6.09445305e+01,  2.56144314e+01,  6.63953900e-01],\n        [ 7.11565170e+01,  6.25597382e+00,  5.40939271e-01],\n        [ 4.14133072e+01,  7.08062935e+00,  6.08310282e-01],\n        ...,\n        [ 3.40217209e+01,  6.89691544e+01,  1.27843499e-01],\n        [ 7.28035126e+01,  1.43601318e+02,  9.21786129e-02],\n        [ 7.27402191e+01,  1.56649353e+02,  8.54525268e-02]]],\n      dtype=float32), scores=array([0.03933325, 0.03505552, 0.03478917, 0.03024065, 0.02591768,\n       0.0258736 , 0.02582949, 0.02570817, 0.02539107, 0.02532637,\n       0.02380559, 0.02378765, 0.02272761, 0.02252334, 0.02047765,\n       0.02019939, 0.02014679, 0.0199365 , 0.01971588, 0.01966649],\n      dtype=float32), bboxes_xyxy=array([[ 8.1108749e+01,  3.6276523e+02,  1.0630549e+02,  4.0783939e+02],\n       [ 9.3472572e+01,  3.3206680e+02,  1.3425014e+02,  4.0829224e+02],\n       [ 3.9273932e+02,  4.9263901e+02,  4.5511844e+02,  5.7135815e+02],\n       [ 1.5023174e+01,  2.8129398e+02,  5.1180389e+01,  3.3697476e+02],\n       [ 5.6432697e+02,  5.1840320e+02,  6.3217261e+02,  6.5255615e+02],\n       [ 5.3950250e+02,  3.7114255e+02,  5.7731787e+02,  4.9819284e+02],\n       [-3.1652374e+00, -6.4132576e+00,  1.4111818e+02,  9.7091354e+01],\n       [-2.0787670e+01, -5.7331018e+00,  1.0649762e+02,  7.0466354e+01],\n       [-3.2757523e+01, -7.1842690e+00,  1.1137049e+02,  1.4850111e+02],\n       [-4.5728931e+00,  1.9842491e+02,  4.1110657e+01,  2.7105209e+02],\n       [-2.8538303e+00, -6.6975800e+01,  9.0169113e+01,  2.5032312e+02],\n       [-8.0334702e+00, -2.6710453e+00,  3.3494434e+02,  3.0032532e+02],\n       [ 5.7719916e+02,  3.6678748e+02,  6.4350659e+02,  5.3759894e+02],\n       [-3.9946175e+00, -4.1155067e+01,  1.0497908e+02,  2.0189648e+02],\n       [ 1.2046942e+02,  3.4093637e+02,  1.4555463e+02,  3.9695294e+02],\n       [ 2.9441208e+02,  1.0311748e+02,  3.1204697e+02,  1.2857637e+02],\n       [ 5.3274115e+02,  1.0293113e+02,  5.7991742e+02,  2.4592476e+02],\n       [ 2.3719573e-01,  2.9826004e+02,  1.6348869e+01,  4.3033258e+02],\n       [ 4.2165382e+01,  3.0568152e+02,  7.1513069e+01,  3.4827820e+02],\n       [-2.6403000e+01, -5.0337410e+01,  1.0633637e+02,  7.5845764e+01]],\n      dtype=float32), edge_links=array([], shape=(0, 2), dtype=float64), edge_colors=array([], shape=(0, 3), dtype=float64), keypoint_colors=array([[145,  64, 173],\n       [151,  52, 145],\n       [111, 190, 239],\n       [121, 122, 148],\n       [111,  34,  75],\n       [ 19, 141, 109],\n       [ 62,  53,  44],\n       [168, 164, 148],\n       [200,  43,  95],\n       [ 13, 192,  83],\n       [ 30,   2, 199],\n       [ 13, 122,  86],\n       [ 46,  68,  21],\n       [ 45, 222,  27],\n       [ 90, 246, 101],\n       [153,  45, 110],\n       [149,  42,   0]], dtype=uint8), image_shape=(640, 640))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:48.466221700Z",
     "start_time": "2023-10-25T13:20:48.394260400Z"
    }
   },
   "id": "a69bb6a4828c5959"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "PoseEstimationPrediction(poses=array([[[ 7.29603729e+01,  2.90555450e+02,  2.40387648e-01],\n        [ 6.90669861e+01,  2.89268494e+02,  2.12954700e-01],\n        [ 7.31052017e+01,  2.89508423e+02,  2.05087841e-01],\n        ...,\n        [ 7.72542801e+01,  3.20757385e+02,  2.41989732e-01],\n        [ 6.98841553e+01,  3.26351593e+02,  1.46408051e-01],\n        [ 7.87367096e+01,  3.26239441e+02,  1.42147750e-01]],\n\n       [[ 9.27663345e+01,  2.70622772e+02,  4.15875465e-01],\n        [ 9.24449005e+01,  2.68740509e+02,  2.89114654e-01],\n        [ 9.14170761e+01,  2.68784576e+02,  4.05241132e-01],\n        ...,\n        [ 9.29963531e+01,  3.23557800e+02,  2.14025617e-01],\n        [ 8.40725555e+01,  3.32570709e+02,  1.13747567e-01],\n        [ 9.17022324e+01,  3.32389832e+02,  1.11210644e-01]],\n\n       [[ 3.24714935e+02,  4.01931946e+02,  2.96225727e-01],\n        [ 3.26583557e+02,  3.98916443e+02,  3.20348173e-01],\n        [ 3.26441833e+02,  3.98800873e+02,  1.34563357e-01],\n        ...,\n        [ 3.38393250e+02,  4.64557373e+02,  1.79093242e-01],\n        [ 3.24118225e+02,  4.88062561e+02,  1.60950810e-01],\n        [ 3.35902832e+02,  4.87887848e+02,  1.30747378e-01]],\n\n       ...,\n\n       [[ 4.93706179e+00,  2.49846313e+02,  2.33334541e-01],\n        [ 3.56948829e+00,  2.47394852e+02,  1.46367788e-01],\n        [ 4.61053133e+00,  2.47610001e+02,  2.34512687e-01],\n        ...,\n        [-1.53615326e-01,  2.79262146e+02,  2.33991444e-01],\n        [ 1.70266259e+00,  3.04532776e+02,  1.92468643e-01],\n        [-5.25087774e-01,  3.04796661e+02,  2.05798358e-01]],\n\n       [[ 4.88474350e+01,  2.49574112e+02,  4.68091160e-01],\n        [ 4.99099503e+01,  2.47521469e+02,  3.08891773e-01],\n        [ 4.64255371e+01,  2.47636993e+02,  4.79910582e-01],\n        ...,\n        [ 4.07141228e+01,  2.80658508e+02,  1.30137980e-01],\n        [ 4.91870766e+01,  2.87034271e+02,  9.61355567e-02],\n        [ 4.18487968e+01,  2.87851379e+02,  9.76549983e-02]],\n\n       [[ 4.76129150e+01,  2.00112743e+01,  6.63953900e-01],\n        [ 5.55910301e+01,  4.88747978e+00,  5.40939271e-01],\n        [ 3.23541451e+01,  5.53174162e+00,  6.08310282e-01],\n        ...,\n        [ 2.65794697e+01,  5.38821526e+01,  1.27843499e-01],\n        [ 5.68777428e+01,  1.12188530e+02,  9.21786129e-02],\n        [ 5.68282967e+01,  1.22382309e+02,  8.54525268e-02]]],\n      dtype=float32), scores=array([0.03933325, 0.03505552, 0.03478917, 0.03024065, 0.02591768,\n       0.0258736 , 0.02582949, 0.02570817, 0.02539107, 0.02532637,\n       0.02380559, 0.02378765, 0.02272761, 0.02252334, 0.02047765,\n       0.02019939, 0.02014679, 0.0199365 , 0.01971588, 0.01966649],\n      dtype=float32), bboxes_xyxy=array([[ 6.33662109e+01,  2.83410339e+02,  8.30511627e+01,\n         3.18624512e+02],\n       [ 7.30254440e+01,  2.59427185e+02,  1.04882919e+02,\n         3.18978302e+02],\n       [ 3.06827606e+02,  3.84874237e+02,  3.55561279e+02,\n         4.46373566e+02],\n       [ 1.17368546e+01,  2.19760925e+02,  3.99846802e+01,\n         2.63261536e+02],\n       [ 4.40880432e+02,  4.05002502e+02,  4.93884857e+02,\n         5.09809509e+02],\n       [ 4.21486328e+02,  2.89955109e+02,  4.51029602e+02,\n         3.89213165e+02],\n       [-2.47284174e+00, -5.01035738e+00,  1.10248581e+02,\n         7.58526230e+01],\n       [-1.62403679e+01, -4.47898579e+00,  8.32012634e+01,\n         5.50518379e+01],\n       [-2.55918140e+01, -5.61271000e+00,  8.70081940e+01,\n         1.16016495e+02],\n       [-3.57257271e+00,  1.55019455e+02,  3.21176987e+01,\n         2.11759445e+02],\n       [-2.22955489e+00, -5.23248444e+01,  7.04446182e+01,\n         1.95564941e+02],\n       [-6.27614880e+00, -2.08675408e+00,  2.61675262e+02,\n         2.34629150e+02],\n       [ 4.50936829e+02,  2.86552704e+02,  5.02739532e+02,\n         4.19999176e+02],\n       [-3.12079477e+00, -3.21523972e+01,  8.20149078e+01,\n         1.57731628e+02],\n       [ 9.41167374e+01,  2.66356537e+02,  1.13714554e+02,\n         3.10119476e+02],\n       [ 2.30009430e+02,  8.05605316e+01,  2.43786697e+02,\n         1.00450287e+02],\n       [ 4.16204010e+02,  8.04149475e+01,  4.53060486e+02,\n         1.92128723e+02],\n       [ 1.85309172e-01,  2.33015656e+02,  1.27725544e+01,\n         3.36197327e+02],\n       [ 3.29417038e+01,  2.38813690e+02,  5.58695869e+01,\n         2.72092346e+02],\n       [-2.06273441e+01, -3.93261032e+01,  8.30752945e+01,\n         5.92545013e+01]], dtype=float32), edge_links=array([], shape=(0, 2), dtype=float64), edge_colors=array([], shape=(0, 3), dtype=float64), keypoint_colors=array([[145,  64, 173],\n       [151,  52, 145],\n       [111, 190, 239],\n       [121, 122, 148],\n       [111,  34,  75],\n       [ 19, 141, 109],\n       [ 62,  53,  44],\n       [168, 164, 148],\n       [200,  43,  95],\n       [ 13, 192,  83],\n       [ 30,   2, 199],\n       [ 13, 122,  86],\n       [ 46,  68,  21],\n       [ 45, 222,  27],\n       [ 90, 246, 101],\n       [153,  45, 110],\n       [149,  42,   0]], dtype=uint8), image_shape=(640, 640))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = image_processor.postprocess_predictions(p, metadata)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:20:48.466221700Z",
     "start_time": "2023-10-25T13:20:48.405449Z"
    }
   },
   "id": "b156171ef5eb49ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 748/5000 [01:12<06:51, 10.34it/s]"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for image_file in tqdm(image_files):\n",
    "    image_original = cv2.imread(image_file)[..., ::-1]  # BGR -> RGB\n",
    "\n",
    "    # Resize image to 640x640\n",
    "    image_resized, metadata = image_processor.preprocess_image(image_original)\n",
    "    model_input = np.expand_dims(image_resized, 0)  # [1,3,640,640]\n",
    "    [flat_predictions] = session.run(outputs, {inputs[0]: model_input})\n",
    "\n",
    "    pred_bboxes = flat_predictions[:, 1:5]\n",
    "    pred_scores = flat_predictions[:, 5]\n",
    "    pred_joints = flat_predictions[:, 6:].reshape(len(pred_bboxes), -1, 3)\n",
    "\n",
    "    p = PoseEstimationPrediction(\n",
    "        poses=pred_joints,\n",
    "        scores=pred_scores,\n",
    "        bboxes_xyxy=pred_bboxes,\n",
    "        edge_links=np.zeros((0,2)),\n",
    "        edge_colors=np.zeros((0,3)),\n",
    "        keypoint_colors=np.random.randint(0, 255, (pred_joints.shape[1], 3), dtype=np.uint8),\n",
    "        image_shape=(model_input.shape[2], model_input.shape[3]),\n",
    "    )\n",
    "    p = image_processor.postprocess_predictions(p, metadata)\n",
    "\n",
    "    predictions.append(PoseEstimationPredictions(\n",
    "        poses=p.poses,\n",
    "        scores=p.scores,\n",
    "        bboxes_xyxy=p.bboxes_xyxy,\n",
    "    ))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-25T13:20:48.425388700Z"
    }
   },
   "id": "e939c4ad4ae9d3c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "import json_tricks as json\n",
    "import collections\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def predictions_to_coco(predictions, image_files):\n",
    "    predicted_poses = []\n",
    "    predicted_scores = []\n",
    "    non_empty_image_ids = []\n",
    "    for image_file, image_predictions in zip(image_files, predictions):\n",
    "        non_empty_image_ids.append(int(os.path.splitext(os.path.basename(image_file))[0]))\n",
    "        predicted_poses.append(image_predictions.poses)\n",
    "        predicted_scores.append(image_predictions.scores)\n",
    "\n",
    "    coco_pred = _coco_convert_predictions_to_dict(predicted_poses, predicted_scores, non_empty_image_ids)\n",
    "    return coco_pred\n",
    "\n",
    "\n",
    "def _coco_process_keypoints(keypoints):\n",
    "    tmp = keypoints.copy()\n",
    "    if keypoints[:, 2].max() > 0:\n",
    "        num_keypoints = keypoints.shape[0]\n",
    "        for i in range(num_keypoints):\n",
    "            tmp[i][0:3] = [float(keypoints[i][0]), float(keypoints[i][1]), float(keypoints[i][2])]\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _coco_convert_predictions_to_dict(predicted_poses, predicted_scores, image_ids):\n",
    "    kpts = collections.defaultdict(list)\n",
    "    for poses, scores, image_id_int in zip(predicted_poses, predicted_scores, image_ids):\n",
    "\n",
    "        for person_index, kpt in enumerate(poses):\n",
    "            area = (np.max(kpt[:, 0]) - np.min(kpt[:, 0])) * (np.max(kpt[:, 1]) - np.min(kpt[:, 1]))\n",
    "            kpt = _coco_process_keypoints(kpt)\n",
    "            kpts[image_id_int].append(\n",
    "                {\"keypoints\": kpt[:, 0:3], \"score\": float(scores[person_index]), \"image\": image_id_int, \"area\": area})\n",
    "\n",
    "    oks_nmsed_kpts = []\n",
    "    # image x person x (keypoints)\n",
    "    for img in kpts.keys():\n",
    "        # person x (keypoints)\n",
    "        img_kpts = kpts[img]\n",
    "        # person x (keypoints)\n",
    "        # do not use nms, keep all detections\n",
    "        keep = []\n",
    "        if len(keep) == 0:\n",
    "            oks_nmsed_kpts.append(img_kpts)\n",
    "        else:\n",
    "            oks_nmsed_kpts.append([img_kpts[_keep] for _keep in keep])\n",
    "\n",
    "    classes = [\"__background__\", \"person\"]\n",
    "    _class_to_coco_ind = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "    data_pack = [\n",
    "        {\"cat_id\": _class_to_coco_ind[cls], \"cls_ind\": cls_ind, \"cls\": cls, \"ann_type\": \"keypoints\",\n",
    "         \"keypoints\": oks_nmsed_kpts}\n",
    "        for cls_ind, cls in enumerate(classes)\n",
    "        if not cls == \"__background__\"\n",
    "    ]\n",
    "\n",
    "    results = _coco_keypoint_results_one_category_kernel(data_pack[0], num_joints=17)\n",
    "    return results\n",
    "\n",
    "\n",
    "def _coco_keypoint_results_one_category_kernel(data_pack, num_joints: int):\n",
    "    cat_id = data_pack[\"cat_id\"]\n",
    "    keypoints = data_pack[\"keypoints\"]\n",
    "    cat_results = []\n",
    "\n",
    "    for img_kpts in keypoints:\n",
    "        if len(img_kpts) == 0:\n",
    "            continue\n",
    "\n",
    "        _key_points = np.array([img_kpts[k][\"keypoints\"] for k in range(len(img_kpts))])\n",
    "        key_points = np.zeros((_key_points.shape[0], num_joints * 3), dtype=np.float32)\n",
    "\n",
    "        for ipt in range(num_joints):\n",
    "            key_points[:, ipt * 3 + 0] = _key_points[:, ipt, 0]\n",
    "            key_points[:, ipt * 3 + 1] = _key_points[:, ipt, 1]\n",
    "            # keypoints score.\n",
    "            key_points[:, ipt * 3 + 2] = _key_points[:, ipt, 2]\n",
    "\n",
    "        for k in range(len(img_kpts)):\n",
    "            kpt = key_points[k].reshape((num_joints, 3))\n",
    "            left_top = np.amin(kpt, axis=0)\n",
    "            right_bottom = np.amax(kpt, axis=0)\n",
    "\n",
    "            w = right_bottom[0] - left_top[0]\n",
    "            h = right_bottom[1] - left_top[1]\n",
    "\n",
    "            cat_results.append(\n",
    "                {\n",
    "                    \"image_id\": img_kpts[k][\"image\"],\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"keypoints\": list(key_points[k]),\n",
    "                    \"score\": img_kpts[k][\"score\"],\n",
    "                    \"bbox\": list([left_top[0], left_top[1], w, h]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return cat_results\n",
    "\n",
    "\n",
    "coco_pred = predictions_to_coco(predictions, image_files)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    res_file = os.path.join(td, \"keypoints_coco2017_results.json\")\n",
    "\n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump(coco_pred, f)\n",
    "\n",
    "    coco_dt = copy.deepcopy(gt)\n",
    "    coco_dt = coco_dt.loadRes(res_file)\n",
    "\n",
    "    coco_evaluator = COCOeval(gt, coco_dt, iouType=\"keypoints\")\n",
    "    coco_evaluator.evaluate()  # run per image evaluation\n",
    "    coco_evaluator.accumulate()  # accumulate per image results\n",
    "    coco_evaluator.summarize()  # display summary metrics of results"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a077a3e5d1ed0342"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coco_evaluator.stats"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bc3ab706ff4eeb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7f1f02de114a3ed4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
