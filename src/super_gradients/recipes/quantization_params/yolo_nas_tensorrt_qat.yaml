ptq_only: False                 # whether to launch QAT, or leave PTQ only
quantizer:
  TRTQuantizer:
    selective_quantizer_params:
      calibrator_w: "max"        # calibrator type for weights, acceptable types are ["max", "histogram"]
      calibrator_i: "histogram"  # calibrator type for inputs acceptable types are ["max", "histogram"]
      per_channel: True          # per-channel quantization of weights, activations stay per-tensor by default
      learn_amax: False          # enable learnable amax in all TensorQuantizers using straight-through estimator
      skip_modules:              # optional list of module names (strings) to skip from quantization
        - heads.head1.reg_convs
        - heads.head1.cls_convs
        - heads.head1.cls_pred
        - heads.head1.reg_pred

        - heads.head2.reg_convs
        - heads.head2.cls_convs
        - heads.head2.cls_pred
        - heads.head2.reg_pred

        - heads.head3.reg_convs
        - heads.head3.cls_convs
        - heads.head3.cls_pred
        - heads.head3.reg_pred

    calib_params:
      histogram_calib_method: "percentile"  # calibration method for all "histogram" calibrators, acceptable types are ["percentile", "entropy", "mse"], "max" calibrators always use "max"
      percentile: 99.99                     # percentile for all histogram calibrators with method "percentile", other calibrators are not affected
      num_calib_batches: 128                # number of batches to use for calibration, if None, 512 / batch_size will be used
      verbose: False                        # if calibrator should be verbose

    qat_params:
      batch_size_divisor: 2                 # Divisor used to calculate the batch size. Default value is 2.
      max_epochs_divisor: 10                # Divisor used to calculate the maximum number of epochs. Default value is 10.
      lr_decay_factor: 0.01                 # Factor used to decay the learning rate, weight decay and warmup. Default value is 0.01.
      warmup_epochs_divisor: 10             #  Divisor used to calculate the number of warm-up epochs. Default value is 10.
      cosine_final_lr_ratio: 0.01           # Ratio used to determine the final learning rate in a cosine annealing schedule. Default value is 0.01.
      disable_phase_callbacks: True         # Flag to control to disable phase callbacks, which can interfere with QAT. Default value is True.
      disable_augmentations: False          # Flag to control to disable phase augmentations, which can interfere with QAT. Default value is False.


exporter:
  TRTExporter:
    output_path: yolo_nas_trt_qat_int8.onnx
