{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pose Estimation Fine Tuninig with Super Gradients\n",
    "\n",
    "In this tutorial notebook we demonstrate how to fine tune a pose estimation model using SuperGradients. It is recommended that you go over [Pose Estimation tutorial](https://docs.deci.ai/super-gradients/documentation/source/PoseEstimation.html) docs first to get familiar with terminology and concepts we use here.\n",
    "\n",
    "From this tutorial you will learn:\n",
    "* How to implement a custom dataset class for pose estimation task\n",
    "* How to instantiate a pre-trained pose estimation model and change number of joints it predicts to fit your dataset\n",
    "* How to fine-tune a pose estimation model using SuperGradients\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement super_gradients==3.2 (from versions: 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.5.2, 1.6.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 2.0.0, 2.0.1, 2.1.0, 2.2.0, 2.5.0, 2.6.0, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.0.4, 3.0.5, 3.0.6, 3.0.7, 3.0.8, 3.0.9, 3.1.0, 3.1.1)\n",
      "ERROR: No matching distribution found for super_gradients==3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install super_gradients==3.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:49:56.029459900Z",
     "start_time": "2023-06-02T12:49:52.676065400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "The first thing we need is our dataset. For this tutorial we will be using [Animals Pose](https://sites.google.com/view/animal-pose/) dataset. It is a relatively small dataset of 6K+ instances of animals, each annotated with 20 keypoints.\n",
    "\n",
    "![](https://lh6.googleusercontent.com/hehW9yRzdcniQ2i1Ts65ceGERa70cBbaLlRixxu7HlUMHabt8HdgcxutG4vmVOas-U1h6g=w16383)\n",
    "\n",
    "You need to download the datasets from these locations and update the paths below accordingly:\n",
    "\n",
    "* https://drive.google.com/drive/folders/1xxm6ZjfsDSmv6C9JvbgiGrmHktrUjV5x?usp=sharing\n",
    "* https://drive.google.com/drive/folders/1-yOSGWts2ZDYFx29u7vPcX4CdGJkPx1w?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ANIMALS_POSE_DATA_DIR = \"e:/animalpose\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:49:56.104985400Z",
     "start_time": "2023-06-02T12:49:56.033460900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Animals Pose dataset uses COCO-style annotation format.\n",
    "Unfortunately it's not 100% compatible with COCO parser from pycocotools.\n",
    "So we have to write out own parser. That is not a big issue, since the format is very simple:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"images\": {\n",
    "        \"1\": \"2007_000063.jpg\",\n",
    "        \"2\": \"2007_000175.jpg\",\n",
    "        \"6\": \"2007_000491.jpg\",\n",
    "        ...\n",
    "        \"4606\": \"sh97.jpg\",\n",
    "        \"4607\": \"sh98.jpeg\",\n",
    "        \"4608\": \"sh99.jpeg\"\n",
    "    },\n",
    "    \"annotations\": [\n",
    "        {\n",
    "            \"image_id\": 1,\n",
    "            \"bbox\": [\n",
    "                123,\n",
    "                115,\n",
    "                379,\n",
    "                275\n",
    "            ],\n",
    "            \"keypoints\": [\n",
    "                [193, 216, 1],\n",
    "                [160, 217, 1],\n",
    "                [174, 261, 1],\n",
    "                [204, 186, 1],\n",
    "                [152, 182, 1],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [273, 168, 1],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [266, 225, 1],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 0],\n",
    "                [190, 145, 1],\n",
    "                [351, 238, 1]\n",
    "            ],\n",
    "            \"num_keypoints\": 20,\n",
    "            \"category_id\": 1\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\n",
    "            \"supercategory\": \"animal\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"dog\",\n",
    "            \"keypoints\": [\n",
    "                \"left_eye\",\n",
    "                \"right_eye\",\n",
    "                \"nose\",\n",
    "                \"left_ear\",\n",
    "                \"right_ear\",\n",
    "                \"left_front_elbow\",\n",
    "                \"right_front_elbow\",\n",
    "                \"left_back_elbow\",\n",
    "                \"right_back_elbow\",\n",
    "                \"left_front_knee\",\n",
    "                \"right_front_knee\",\n",
    "                \"left_back_knee\",\n",
    "                \"right_back_knee\",\n",
    "                \"left_front_paw\",\n",
    "                \"right_front_paw\",\n",
    "                \"left_back_paw\",\n",
    "                \"right_back_paw\",\n",
    "                \"throat\",\n",
    "                \"withers\",\n",
    "                \"tailbase\"\n",
    "            ],\n",
    "            \"skeleton\": [\n",
    "                [0, 1],\n",
    "                [0, 2],\n",
    "                [1, 2],\n",
    "                [0, 3],\n",
    "                [1, 4],\n",
    "                [2, 17],\n",
    "                [18, 19],\n",
    "                [5, 9],\n",
    "                [6, 10],\n",
    "                [7, 11],\n",
    "                [8, 12],\n",
    "                [9, 13],\n",
    "                [10, 14],\n",
    "                [11, 15],\n",
    "                [12, 16]\n",
    "            ]\n",
    "        },\n",
    "```\n",
    "\n",
    "To train pose estimation model using Super Gradients we need to implement a custom dataset class that will parse this format and return images and targets for the model.\n",
    "Fortunately, Super Gradients provides a base class for pose estimation datasets that handles most of the boilerplate code for us.\n",
    "\n",
    "We need to implement is dataset parsing method that will return annotations in a format that Super Gradients expects.\n",
    "The dataset class is expected to return a tuple of the following objects:\n",
    "\n",
    "```python\n",
    "class AnimalsPoseDataset:\n",
    "    def __getitem__(self, index):\n",
    "        ...\n",
    "        return image, targets, {\"gt_joints\": gt_joints, \"gt_bboxes\": gt_bboxes, \"gt_iscrowd\": gt_iscrowd, \"gt_areas\": gt_areas}\n",
    "```\n",
    "Return values are:\n",
    "* image - torch tensor of [C,H,W] shape that represents an input image to the model\n",
    "* targets - model-specific targets to train the model itself. Fortunately SG will take care of generating these targets for us. Our goal is to provide the keypoints of [Num Instances, Num Joints, 3] shape.\n",
    "* extras - Additional information with poses for metric computation. Must be a dictionary with following keys:\n",
    "    * gt_joints - Array of keypoints for all poses in the image. Numpy array of [Num Instances, Num Joints, 3] shape\n",
    "    * gt_bboxes - Array of bounding boxes for each pose in the image. Numpy array of [Num Instances, 4] shape\n",
    "    * gt_iscrowd - Array of iscrowd flags for each pose in the image. Numpy array of [Num Instances] shape\n",
    "    * gt_areas - Array of areas for each skeleton in the image. Numpy array of [Num Instances] shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into C:\\Users\\blood\\sg_logs\\console.log\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Mapping, Any, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "from super_gradients.common.decorators.factory_decorator import resolve_param\n",
    "from super_gradients.common.factories.target_generator_factory import TargetGeneratorsFactory\n",
    "from super_gradients.common.factories.transforms_factory import TransformsFactory\n",
    "from super_gradients.training.datasets.pose_estimation_datasets import BaseKeypointsDataset\n",
    "from super_gradients.training.transforms.keypoint_transforms import KeypointTransform\n",
    "\n",
    "\n",
    "class AnimalPoseKeypointsDataset(BaseKeypointsDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for training pose estimation models on COCO Keypoints dataset.\n",
    "    Use should pass a target generator class that is model-specific and generates the targets for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    @resolve_param(\"transforms\", TransformsFactory())\n",
    "    @resolve_param(\"target_generator\", TargetGeneratorsFactory())\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        images_dir: str,\n",
    "        json_file: str,\n",
    "        include_empty_samples: bool,\n",
    "        target_generator,\n",
    "        transforms: List[KeypointTransform],\n",
    "        min_instance_area: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data_dir: Root directory of the COCO dataset\n",
    "        :param images_dir: path suffix to the images directory inside the dataset_root\n",
    "        :param json_file: path suffix to the json file inside the dataset_root\n",
    "        :param include_empty_samples: if True, images without any annotations will be included in the dataset.\n",
    "            Otherwise, they will be filtered out.\n",
    "        :param target_generator: Target generator that will be used to generate the targets for the model.\n",
    "            See DEKRTargetsGenerator for an example.\n",
    "        :param transforms: Transforms to be applied to the image & keypoints\n",
    "        :param min_instance_area: Minimum area of an instance to be included in the dataset\n",
    "        \"\"\"\n",
    "        super().__init__(transforms=transforms, target_generator=target_generator, min_instance_area=min_instance_area)\n",
    "\n",
    "        with open(os.path.join(data_dir, json_file), \"r\") as f:\n",
    "            json_annotations = json.load(f)\n",
    "\n",
    "        self.joints = json_annotations[\"categories\"][0][\"keypoints\"]\n",
    "        self.num_joints = len(self.joints)\n",
    "\n",
    "        images_and_ids = [(image_id, os.path.join(data_dir, images_dir, image_path)) for image_id, image_path in json_annotations[\"images\"].items()]\n",
    "        self.image_ids, self.image_files = zip(*images_and_ids)\n",
    "\n",
    "        self.annotations = []\n",
    "\n",
    "        for image_id in self.image_ids:\n",
    "            keypoints_per_image = []\n",
    "            bboxes_per_image = []\n",
    "\n",
    "            image_annotations = [ann for ann in json_annotations[\"annotations\"] if str(ann[\"image_id\"]) == str(image_id)]\n",
    "            for ann in image_annotations:\n",
    "                keypoints = np.array(ann[\"keypoints\"]).reshape(self.num_joints, 3)\n",
    "                bbox = np.array(ann[\"bbox\"])\n",
    "                keypoints_per_image.append(keypoints)\n",
    "                bboxes_per_image.append(bbox)\n",
    "\n",
    "            keypoints_per_image = np.array(keypoints_per_image, dtype=np.float32).reshape(-1, self.num_joints, 3)\n",
    "            bboxes_per_image = np.array(bboxes_per_image, dtype=np.float32).reshape(-1, 4)\n",
    "            annotation = keypoints_per_image, bboxes_per_image\n",
    "            self.annotations.append(annotation)\n",
    "\n",
    "        # if not include_empty_samples:\n",
    "        #     subset = [img_id for img_id in self.ids if len(self.coco.getAnnIds(imgIds=img_id, iscrowd=None)) > 0]\n",
    "        #     self.ids = subset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def load_sample(self, index):\n",
    "        file_path = self.image_files[index]\n",
    "        keypoints, boxes = self.annotations[index] # boxes in xyxy format\n",
    "\n",
    "        gt_areas = np.array([(xmax-xmin) * (ymax-ymin) for (xmin, ymin, xmax, ymax) in boxes], dtype=np.float32)\n",
    "        gt_iscrowd = np.array([0] * len(keypoints), dtype=bool)\n",
    "\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "        mask = np.zeros(image.shape[:2], dtype=np.float32)\n",
    "\n",
    "        return image, mask, keypoints, gt_areas, boxes, gt_iscrowd\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Any, Mapping[str, Any]]:\n",
    "        img, mask, gt_joints, gt_areas, gt_bboxes, gt_iscrowd = self.load_sample(index)\n",
    "        img, mask, gt_joints, gt_areas, gt_bboxes = self.transforms(img, mask, gt_joints, areas=gt_areas, bboxes=gt_bboxes)\n",
    "\n",
    "        image_shape = img.size(1), img.size(2)\n",
    "        gt_joints, gt_areas, gt_bboxes, gt_iscrowd = self.filter_joints(image_shape, gt_joints, gt_areas, gt_bboxes, gt_iscrowd)\n",
    "\n",
    "        targets = self.target_generator(img, gt_joints, mask)\n",
    "        return img, targets, {\"gt_joints\": gt_joints, \"gt_bboxes\": gt_bboxes, \"gt_iscrowd\": gt_iscrowd, \"gt_areas\": gt_areas}\n",
    "\n",
    "\n",
    "    def filter_joints(\n",
    "        self,\n",
    "        image_shape,\n",
    "        joints: np.ndarray,\n",
    "        areas: np.ndarray,\n",
    "        bboxes: np.ndarray,\n",
    "        is_crowd: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Filter instances that are either too small or do not have visible keypoints.\n",
    "\n",
    "        :param image: Image if [H,W,C] shape. Used to infer image boundaries\n",
    "        :param joints: Array of shape [Num Instances, Num Joints, 3]\n",
    "        :param areas: Array of shape [Num Instances] with area of each instance.\n",
    "                      Instance area comes from segmentation mask from COCO annotation file.\n",
    "        :param bboxes: Array of shape [Num Instances, 4] for bounding boxes in XYWH format.\n",
    "                       Bounding boxes comes from segmentation mask from COCO annotation file.\n",
    "        :param: is_crowd: Array of shape [Num Instances] indicating whether an instance is a crowd target.\n",
    "        :return: [New Num Instances, Num Joints, 3], New Num Instances <= Num Instances\n",
    "        \"\"\"\n",
    "\n",
    "        # Update visibility of joints for those that are outside the image\n",
    "        outside_image_mask = (joints[:, :, 0] < 0) | (joints[:, :, 1] < 0) | (joints[:, :, 0] >= image_shape[1]) | (joints[:, :, 1] >= image_shape[0])\n",
    "        joints[outside_image_mask, 2] = 0\n",
    "\n",
    "        # Filter instances with all invisible keypoints\n",
    "        instances_with_visible_joints = np.count_nonzero(joints[:, :, 2], axis=-1) > 0\n",
    "        instances_with_good_area = areas > self.min_instance_area\n",
    "\n",
    "        keep_mask = instances_with_visible_joints & instances_with_good_area\n",
    "\n",
    "        joints = joints[keep_mask]\n",
    "        areas = areas[keep_mask]\n",
    "        bboxes = bboxes[keep_mask]\n",
    "        is_crowd = is_crowd[keep_mask]\n",
    "\n",
    "        return joints, areas, bboxes, is_crowd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:04.672707600Z",
     "start_time": "2023-06-02T12:49:56.066015100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, when we have the dataset class we can leverage existing dataset configs for COCO and use transforms & target generator from these configs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from super_gradients.training.dataloaders import get_data_loader\n",
    "\n",
    "def animalpose_pose_train(dataset_params: Dict = None, dataloader_params: Dict = None) -> DataLoader:\n",
    "    return get_data_loader(\n",
    "        config_name=\"coco_pose_estimation_dekr_dataset_params\",\n",
    "        dataset_cls=AnimalPoseKeypointsDataset,\n",
    "        train=True,\n",
    "        dataset_params=dataset_params,\n",
    "        dataloader_params=dataloader_params,\n",
    "    )\n",
    "\n",
    "\n",
    "def animalpose_pose_val(dataset_params: Dict = None, dataloader_params: Dict = None) -> DataLoader:\n",
    "    return get_data_loader(\n",
    "        config_name=\"coco_pose_estimation_dekr_dataset_params\",\n",
    "        dataset_cls=AnimalPoseKeypointsDataset,\n",
    "        train=False,\n",
    "        dataset_params=dataset_params,\n",
    "        dataloader_params=dataloader_params,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:04.689359500Z",
     "start_time": "2023-06-02T12:50:04.683353700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are almost ready to instantiate our data loaders. There is only one important nuance that we have to cover. Let's instantiate our train dataset and inspect the transformations that we apply to our samples:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[KeypointsLongestMaxSize(max_height=640, max_width=640, interpolation=1, prob=1.0),\n KeypointsPadIfNeeded(min_height=640, min_width=640, image_pad_value=(127, 127, 127), mask_pad_value=1),\n KeypointsRandomHorizontalFlip(flip_index=[0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15], prob=0.5),\n KeypointsRandomAffineTransform(max_rotation=30, min_scale=0.5, max_scale=2, max_translate=0.2, image_pad_value=(127, 127, 127), mask_pad_value=1, prob=0.75),\n KeypointsImageToTensor(),\n KeypointsImageNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "train_data = animalpose_pose_train(dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR,\n",
    "                                                       images_dir=\"images\",\n",
    "                                                       json_file=\"keypoints.json\"),\n",
    "                                   dataloader_params=dict(num_workers=0, batch_size=8))\n",
    "train_data.dataset.transforms.transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:17.496731900Z",
     "start_time": "2023-06-02T12:50:04.690358800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training we apply several augmentations to our samples. There is one, however that needs out special attention, which is KeypointsRandomHorizontalFlip.\n",
    "When working with object detection and semantic segmentation we can safely skip the fact that left and right in flipped image changing sides.\n",
    "However, in pose estimation of objects that possess vertical symmetry, like animals or humans, we have to take this into account.\n",
    "So when we flip the image we also have to swap left and right keypoints. In order to do this correctly KeypointsRandomHorizontalFlip transform must know the rearrange indices for keypoints.\n",
    "\n",
    "If we look at the JSON data of your annotation file, we can see that the order of keypoints is the following:\n",
    "```\n",
    "# 0 \"left_eye\" -> \"right_eye\": 1\n",
    "# 1 \"right_eye\" -> \"left_eye\": 0\n",
    "# 2 \"nose\" -> \"nose\": 2\n",
    "# 3 \"left_ear\" -> \"right_ear\": 4\n",
    "# 4 \"right_ear\" -> \"left_ear\": 3\n",
    "# 5 \"left_front_elbow\" -> \"right_front_elbow\": 6\n",
    "# 6 \"right_front_elbow\" -> \"left_front_elbow\": 5\n",
    "# 7 \"left_back_elbow\" -> \"right_back_elbow\": 8\n",
    "# 8 \"right_back_elbow\" -> \"left_back_elbow\": 7\n",
    "# 9 \"left_front_knee\" -> \"right_front_knee\": 10\n",
    "# 10 \"right_front_knee\" -> \"left_front_knee\": 9\n",
    "# 11 \"left_back_knee\" -> \"right_back_knee\": 12\n",
    "# 12 \"right_back_knee\" -> \"left_back_knee\": 11\n",
    "# 13 \"left_front_paw\" -> \"right_front_paw\": 14\n",
    "# 14 \"right_front_paw\" -> \"left_front_paw\": 13\n",
    "# 15 \"left_back_paw\" -> \"right_back_paw\": 16\n",
    "# 16 \"right_back_paw\" -> \"left_back_paw\": 15\n",
    "# 17 \"throat\" -> \"throat\": 17\n",
    "# 18 \"withers\" -> \"withers\": 18\n",
    "# 10 \"tailbase\" -> \"tailbase\": 19\n",
    "```\n",
    "\n",
    "So our array of indexes will look like this:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ANIMALS_POSE_FLIP_INDEXES = [1,0,2,4,3,6,5,8,7,10,9,12,11,14,13,16,15,17,18,19]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:17.528832200Z",
     "start_time": "2023-06-02T12:50:17.491665200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to instantiate our data loaders. For training data loader we will pass the transforms explicitly:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from super_gradients.training.transforms.keypoint_transforms import KeypointsLongestMaxSize, KeypointsPadIfNeeded, \\\n",
    "    KeypointsRandomHorizontalFlip, KeypointsRandomAffineTransform, KeypointsImageNormalize, KeypointsImageToTensor\n",
    "\n",
    "train_transforms = [\n",
    "    KeypointsLongestMaxSize(640,640),\n",
    "    KeypointsPadIfNeeded(640,640, image_pad_value=(127,127,127), mask_pad_value=1),\n",
    "    KeypointsRandomHorizontalFlip(ANIMALS_POSE_FLIP_INDEXES),\n",
    "    KeypointsRandomAffineTransform(max_rotation=30, min_scale=0.5, max_scale=2, max_translate=0.2, image_pad_value=(127,127,127), mask_pad_value=1, prob=0.5),\n",
    "    KeypointsImageToTensor(),\n",
    "    KeypointsImageNormalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "]\n",
    "\n",
    "train_data = animalpose_pose_train(dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR,\n",
    "                                                       images_dir=\"images\",\n",
    "                                                       json_file=\"keypoints.json\",\n",
    "                                                       transforms=train_transforms),\n",
    "                                   dataloader_params=dict(num_workers=0, batch_size=8))\n",
    "\n",
    "val_data = animalpose_pose_val(dataset_params=dict(data_dir=ANIMALS_POSE_DATA_DIR,\n",
    "                                                   images_dir=\"images\",\n",
    "                                                   json_file=\"keypoints.json\"),\n",
    "                               dataloader_params=dict(num_workers=0, batch_size=8))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:41.194325300Z",
     "start_time": "2023-06-02T12:50:17.512838500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.3309,  1.4269,  1.9920,  ...,  1.8893,  1.5982,  1.3413],\n          [-0.2342,  1.2385,  1.9578,  ...,  2.0263,  1.9064,  1.7352],\n          [-1.0390,  0.9817,  1.9578,  ...,  2.0777,  2.0777,  2.0263],\n          ...,\n          [ 0.0569,  0.0569,  0.0569,  ...,  0.0569,  0.0569,  0.0569],\n          [ 0.0569,  0.0569,  0.0569,  ...,  0.0569,  0.0569,  0.0569],\n          [ 0.0569,  0.0569,  0.0569,  ...,  0.0569,  0.0569,  0.0569]],\n \n         [[ 0.6429,  1.7458,  2.2710,  ...,  2.2185,  1.9209,  1.6583],\n          [ 0.0651,  1.5707,  2.2710,  ...,  2.3410,  2.2185,  2.0434],\n          [-0.7402,  1.3256,  2.2885,  ...,  2.3410,  2.3410,  2.3060],\n          ...,\n          [ 0.1877,  0.1877,  0.1877,  ...,  0.1877,  0.1877,  0.1877],\n          [ 0.1877,  0.1877,  0.1877,  ...,  0.1877,  0.1877,  0.1877],\n          [ 0.1877,  0.1877,  0.1877,  ...,  0.1877,  0.1877,  0.1877]],\n \n         [[ 1.1585,  2.1520,  2.6226,  ...,  2.4657,  2.1868,  1.9254],\n          [ 0.5834,  1.9603,  2.5877,  ...,  2.5877,  2.4657,  2.3088],\n          [-0.2358,  1.6988,  2.5703,  ...,  2.5877,  2.5877,  2.5529],\n          ...,\n          [ 0.4091,  0.4091,  0.4091,  ...,  0.4091,  0.4091,  0.4091],\n          [ 0.4091,  0.4091,  0.4091,  ...,  0.4091,  0.4091,  0.4091],\n          [ 0.4091,  0.4091,  0.4091,  ...,  0.4091,  0.4091,  0.4091]]]),\n (array([[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         ...,\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32),\n  array([[[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         ...,\n  \n         [[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]]], dtype=float32),\n  array([[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         ...,\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32),\n  array([[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         ...,\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)),\n {'gt_joints': array([[[247.04   , 276.47998,   1.     ],\n          [204.79999, 277.75998,   1.     ],\n          [222.72   , 334.08   ,   1.     ],\n          [261.12   , 238.08   ,   1.     ],\n          [194.56   , 232.95999,   1.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [349.44   , 215.04   ,   1.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [340.47998, 288.     ,   1.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [  0.     ,   0.     ,   0.     ],\n          [243.2    , 185.59999,   1.     ],\n          [449.28   , 304.63998,   1.     ]]], dtype=float32),\n  'gt_bboxes': array([[157.44, 147.2 , 485.12, 352.  ]], dtype=float32),\n  'gt_iscrowd': array([False]),\n  'gt_areas': array([52428.797], dtype=float32)})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.dataset[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:41.266321500Z",
     "start_time": "2023-06-02T12:50:41.195323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[0.0569, 0.0569, 0.0569,  ..., 0.0569, 0.0569, 0.0569],\n          [0.0569, 0.0569, 0.0569,  ..., 0.0569, 0.0569, 0.0569],\n          [0.0569, 0.0569, 0.0569,  ..., 0.0569, 0.0569, 0.0569],\n          ...,\n          [0.0569, 0.0569, 0.0569,  ..., 0.0569, 0.0569, 0.0569],\n          [0.0569, 0.0569, 0.0569,  ..., 0.0569, 0.0569, 0.0569],\n          [0.0569, 0.0569, 0.0569,  ..., 0.0569, 0.0569, 0.0569]],\n \n         [[0.1877, 0.1877, 0.1877,  ..., 0.1877, 0.1877, 0.1877],\n          [0.1877, 0.1877, 0.1877,  ..., 0.1877, 0.1877, 0.1877],\n          [0.1877, 0.1877, 0.1877,  ..., 0.1877, 0.1877, 0.1877],\n          ...,\n          [0.1877, 0.1877, 0.1877,  ..., 0.1877, 0.1877, 0.1877],\n          [0.1877, 0.1877, 0.1877,  ..., 0.1877, 0.1877, 0.1877],\n          [0.1877, 0.1877, 0.1877,  ..., 0.1877, 0.1877, 0.1877]],\n \n         [[0.4091, 0.4091, 0.4091,  ..., 0.4091, 0.4091, 0.4091],\n          [0.4091, 0.4091, 0.4091,  ..., 0.4091, 0.4091, 0.4091],\n          [0.4091, 0.4091, 0.4091,  ..., 0.4091, 0.4091, 0.4091],\n          ...,\n          [0.4091, 0.4091, 0.4091,  ..., 0.4091, 0.4091, 0.4091],\n          [0.4091, 0.4091, 0.4091,  ..., 0.4091, 0.4091, 0.4091],\n          [0.4091, 0.4091, 0.4091,  ..., 0.4091, 0.4091, 0.4091]]]),\n (array([[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         ...,\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32),\n  array([[[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         ...,\n  \n         [[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]],\n  \n         [[0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          ...,\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1],\n          [0.1, 0.1, 0.1, ..., 0.1, 0.1, 0.1]]], dtype=float32),\n  array([[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         ...,\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32),\n  array([[[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         ...,\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]],\n  \n         [[0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.],\n          [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)),\n {'gt_joints': array([[[225.56873 , 287.2361  ,   1.      ],\n          [194.86382 , 297.212   ,   1.      ],\n          [220.05582 , 334.68307 ,   1.      ],\n          [227.67943 , 256.06128 ,   1.      ],\n          [177.76901 , 266.54657 ,   1.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [287.52386 , 220.26797 ,   1.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [296.56213 , 275.6937  ,   1.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [-14.762042, 137.31953 ,   0.      ],\n          [203.30899 , 221.40643 ,   1.      ],\n          [379.916   , 264.62003 ,   1.      ]]], dtype=float32),\n  'gt_bboxes': array([[132.19718914, 107.80214468, 431.09555925, 361.94615569]]),\n  'gt_iscrowd': array([False]),\n  'gt_areas': array([30599.889], dtype=float32)})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:41.349323500Z",
     "start_time": "2023-06-02T12:50:41.260328900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:41.349323500Z",
     "start_time": "2023-06-02T12:50:41.325692800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from super_gradients.common import StrictLoad\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "yolo_nas_pose_l = models.get(Models.YOLO_NAS_POSE_L, num_classes=20, checkpoint_path=\"../pretrained/yolo_nas_pose_l_coco.pth\", strict_load=StrictLoad.KEY_MATCHING)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:59.413808500Z",
     "start_time": "2023-06-02T12:50:41.338325600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "from super_gradients.training.metrics import PoseEstimationMetrics\n",
    "\n",
    "train_params = {\n",
    "    \"average_best_models\":True,\n",
    "    \"warmup_mode\": \"linear_epoch_step\",\n",
    "    \"warmup_initial_lr\": 1e-6,\n",
    "    \"lr_warmup_epochs\": 3,\n",
    "    \"initial_lr\": 5e-4,\n",
    "    \"lr_mode\": \"cosine\",\n",
    "    \"cosine_final_lr_ratio\": 0.1,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"optimizer_params\": {\"weight_decay\": 0.0001},\n",
    "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
    "    \"ema\": False,\n",
    "    \"ema_params\": {\"decay\": 0.9, \"decay_type\": \"threshold\"},\n",
    "    # ONLY TRAINING FOR 10 EPOCHS FOR THIS EXAMPLE NOTEBOOK\n",
    "    \"max_epochs\": 10,\n",
    "    \"mixed_precision\": True,\n",
    "    \"loss\": \"dekr_loss\",\n",
    "    \"criterion_params\": {\n",
    "        \"heatmap_loss\": \"qfl\",\n",
    "        \"heatmap_loss_factor\": 1.0,\n",
    "        \"offset_loss_factor\": 0.1,\n",
    "    },\n",
    "    \"valid_metrics_list\": [\n",
    "        PoseEstimationMetrics(\n",
    "            num_joints=20,\n",
    "            oks_sigmas=None,\n",
    "            max_objects_per_image=30,\n",
    "            post_prediction_callback=yolo_nas_pose_l.get_post_prediction_callback(conf=0.05, iou=0.05),\n",
    "        )\n",
    "    ],\n",
    "    \"metric_to_watch\": 'AP'\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-02T12:50:59.429069200Z",
     "start_time": "2023-06-02T12:50:59.418066700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is now moved to checkpoints\\animal_pose_fine_tuning/console_июнь02_15_50_59.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0: 100%|██████████| 576/576 [10:39<00:00,  1.11s/it, DEKRLoss/heatmap=0.201, DEKRLoss/offset=0.00401, DEKRLoss/total=0.205, gpu_mem=7.05]\n",
      "Validation epoch 0: 100%|██████████| 576/576 [07:24<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 0\n",
      "├── Training\n",
      "│   ├── Dekrloss/heatmap = 0.201\n",
      "│   ├── Dekrloss/offset = 0.004\n",
      "│   └── Dekrloss/total = 0.205\n",
      "└── Validation\n",
      "    ├── Ap = 0.0\n",
      "    ├── Ar = 0.0\n",
      "    ├── Dekrloss/heatmap = 0.1877\n",
      "    ├── Dekrloss/offset = 0.0041\n",
      "    └── Dekrloss/total = 0.1917\n",
      "\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|██████████| 576/576 [10:11<00:00,  1.06s/it, DEKRLoss/heatmap=0.0133, DEKRLoss/offset=0.0033, DEKRLoss/total=0.0166, gpu_mem=7.42] \n",
      "Validation epoch 1:  22%|██▏       | 125/576 [01:09<04:07,  1.82it/s]"
     ]
    }
   ],
   "source": [
    "from super_gradients.training import Trainer\n",
    "\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "trainer = Trainer(experiment_name='animal_pose_fine_tuning', ckpt_root_dir=CHECKPOINT_DIR)\n",
    "\n",
    "trainer.train(model=yolo_nas_pose_l,\n",
    "              training_params=train_params,\n",
    "              train_loader=train_data,\n",
    "              valid_loader=val_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-02T12:50:59.432065200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
