{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Offline evaluation of ONNX Pose Estimation models\n",
    "\n",
    "This example shows how to evaluate **exported** pose estimation models using pycocotools and onnxrunime package.\n",
    "Although we provide a ready-to-use metric class to compute average precision (AP) and average recall (AR) scores, the \n",
    "evaluation protocol during validation is slightly different from what pycocotools suggests for academic evaluation.\n",
    "\n",
    "In particular:\n",
    "\n",
    "## SG\n",
    "\n",
    "* In SG, during training/validation, we resize all images to a fixed size (Default is 640x640) using aspect-ratio preserving resize of the longest size + padding. \n",
    "* Our metric evaluate AP/AR in the resolution of the resized & padded images, **not in the resolution of original image**. \n",
    "\n",
    "\n",
    "## COCOEval\n",
    "\n",
    "* In COCOEval all images are not resized and pose predictions are evaluated in the resolution of original image \n",
    "\n",
    "Because of this discrepancy, metrics reported by `PoseEstimationMetrics` class is usually a bit lower (Usually by ~1AP) than the ones \n",
    "you would get from the same model if computed with COCOEval. \n",
    "\n",
    "For this reason we provide this example to show how you can compute metrics using COCOEval for pose estimation models that are available in SuperGradients."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36fcd2e6daa861c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate the model for evaluation\n",
    "\n",
    "First, let's instantiate the model we are going to evaluate. \n",
    "You can use either pretrained models or provide a checkpoint path to your own trained checkpoint.\n",
    "\n",
    "```python\n",
    "# This is how you can load your custom checkpoint instead of pretrained one\n",
    "model = models.get(\n",
    "    Models.YOLO_NAS_POSE_L,\n",
    "    num_classes=17,\n",
    "    checkpoint_path=\"G:/super-gradients/checkpoints/coco2017_yolo_nas_pose_l_ckpt_best.pth\",\n",
    ")\n",
    "```\n",
    "In this example we will be using pretrained weights for simplicity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33367b7bd09fccbb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-12 12:40:08] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into C:\\Users\\ekhve\\sg_logs\\console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-12 12:40:10] WARNING - redirects.py - NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 12:40:10.951724 19384 redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[2023-10-12 12:40:14] WARNING - env_sanity_check.py - \u001B[31mFailed to verify operating system: Deci officially supports only Linux kernels. Some features may not work as expected.\u001B[0m\n",
      "W1012 12:40:14.608000 19384 env_sanity_check.py:31] \u001B[31mFailed to verify operating system: Deci officially supports only Linux kernels. Some features may not work as expected.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekhve\\.conda\\envs\\sg\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5589: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "\nModel exported successfully to yolo_nas_pose_n.onnx\nModel expects input image of shape [1, 3, 640, 640]\nInput image dtype is torch.uint8\n\nExported model already contains preprocessing (normalization) step, so you don't need to do it manually.\nPreprocessing steps to be applied to input image are:\nSequential(\n  (0): CastTensorTo(dtype=torch.float32)\n  (1): ChannelSelect(channels_indexes=tensor([2, 1, 0], device='cuda:0'))\n  (2): ApplyMeanStd(mean=[0.], scale=[255.])\n)\n\n\nExported model contains postprocessing (NMS) step with the following parameters:\n    num_pre_nms_predictions=1000\n    max_predictions_per_image=20\n    nms_threshold=0.7\n    confidence_threshold=0.01\n    output_predictions_format=flat\n\n\nExported model is in ONNX format and can be used with ONNXRuntime\nTo run inference with ONNXRuntime, please use the following code snippet:\n\n    import onnxruntime\n    import numpy as np\n    session = onnxruntime.InferenceSession(\"yolo_nas_pose_n.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n    inputs = [o.name for o in session.get_inputs()]\n    outputs = [o.name for o in session.get_outputs()]\n\n    example_input_image = np.zeros((1, 3, 640, 640)).astype(np.uint8)\n    predictions = session.run(outputs, {inputs[0]: example_input_image})\n\nExported model can also be used with TensorRT\nTo run inference with TensorRT, please see TensorRT deployment documentation\nYou can benchmark the model using the following code snippet:\n\n    trtexec --onnx=yolo_nas_pose_n.onnx --fp16 --avgRuns=100 --duration=15\n\n\nExported model has predictions in flat format:\n\n# flat_predictions is a 2D array of [N,K] shape\n# Each row represents (image_index, x_min, y_min, x_max, y_max, confidence, joints...)\n# Please note all values are floats, so you have to convert them to integers if needed\n\n[flat_predictions] = predictions\npred_bboxes = flat_predictions[1:5]\npred_scores = flat_predictions[5]\npred_joints = flat_predictions[6:].reshape(-1, 3)\nfor i in range(len(pred_bboxes)):\n    confidence = pred_scores[i]\n    x_min, y_min, x_max, y_max = pred_bboxes[i]\n    print(f\"Detected pose with confidence={{confidence}}, x_min={{x_min}}, y_min={{y_min}}, x_max={{x_max}}, y_max={{y_max}}\")\n    for joint_index, (x, y, confidence) in enumerate(pred_joints[i]):\")\n        print(f\"Joint {{joint_index}} has coordinates x={{x}}, y={{y}}, confidence={{confidence}}\")\n"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from super_gradients.conversion import DetectionOutputFormatMode\n",
    "from super_gradients.common.object_names import Models\n",
    "from super_gradients.training import models\n",
    "\n",
    "model = models.get(\n",
    "    Models.YOLO_NAS_POSE_N,\n",
    "    num_classes=17,\n",
    "    checkpoint_path=\"G:\\super-gradients\\checkpoints\\coco2017_yolo_nas_pose_final\\yolo_nas_pose_n_coco_pose.pth\",\n",
    ").cuda()\n",
    "\n",
    "result = model.export(\n",
    "    \"yolo_nas_pose_n.onnx\",\n",
    "    confidence_threshold=0.01,\n",
    "    max_predictions_per_image=20, nms_threshold=0.7, output_predictions_format=DetectionOutputFormatMode.FLAT_FORMAT\n",
    ")\n",
    "result\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:40:25.420333200Z",
     "start_time": "2023-10-12T09:40:05.863921700Z"
    }
   },
   "id": "36c9acfa4c5057c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare COCO validation data\n",
    "\n",
    "Next, we obtain list of images in COCO2017 validation set and load their annotations.\n",
    "You may want to either set the COCO_ROOT_DIR environment variable where COCO2017 data is located on your machine or edit the default path directylu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372d6ce9605c68f9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['annotations', 'images']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "COCO_DATA_DIR = os.environ.get(\"COCO_ROOT_DIR\", \"g:/coco2017\")\n",
    "os.listdir(COCO_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:40:25.442213100Z",
     "start_time": "2023-10-12T09:40:25.412946900Z"
    }
   },
   "id": "91e2ee3f26130e1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once data is set we can load it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a8be6a43b014149"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:40:25.443223100Z",
     "start_time": "2023-10-12T09:40:25.427353700Z"
    }
   },
   "id": "ca413a0cfd65d3d9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "images_path = os.path.join(COCO_DATA_DIR, \"images/val2017\")\n",
    "image_files = [os.path.join(images_path, x) for x in os.listdir(images_path)]\n",
    "\n",
    "gt_annotations_path = os.path.join(COCO_DATA_DIR, \"annotations/person_keypoints_val2017.json\")\n",
    "gt = COCO(gt_annotations_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T09:40:26.136096300Z",
     "start_time": "2023-10-12T09:40:25.444215300Z"
    }
   },
   "id": "31ef5c6c808fc5c2"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(['onnx::Cast_0'], ['graph2_flat_predictions'])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "session = onnxruntime.InferenceSession(\"yolo_nas_pose_n.onnx\",\n",
    "                                       providers=[\n",
    "                                           \"CUDAExecutionProvider\", \n",
    "                                           \"CPUExecutionProvider\"\n",
    "                                        ])\n",
    "inputs = [o.name for o in session.get_inputs()]\n",
    "outputs = [o.name for o in session.get_outputs()]\n",
    "inputs, outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:11:57.066549700Z",
     "start_time": "2023-10-12T10:11:57.042520400Z"
    }
   },
   "id": "7d20871ce397f6cb"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "ComposeProcessingMetadata(metadata_lst=[RescaleMetadata(original_shape=(500, 375), scale_factor_h=1.28, scale_factor_w=1.28), DetectionPadToSizeMetadata(padding_coordinates=PaddingCoordinates(top=0, bottom=0, left=0, right=160)), None])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from super_gradients.training.utils.predict import PoseEstimationPrediction\n",
    "from super_gradients.module_interfaces import PoseEstimationPredictions\n",
    "import torch\n",
    "from super_gradients.training.processing import ComposeProcessing\n",
    "from super_gradients.training.processing.processing import KeypointsLongestMaxSizeRescale\n",
    "from super_gradients.training.processing.processing import KeypointsBottomRightPadding\n",
    "from super_gradients.training.processing import ImagePermute\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_processor = ComposeProcessing(\n",
    "    [\n",
    "        KeypointsLongestMaxSizeRescale(output_shape=(640, 640)),\n",
    "        KeypointsBottomRightPadding(output_shape=(640, 640), pad_value=127),\n",
    "        ImagePermute(permutation=(2, 0, 1)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_original = cv2.imread(image_files[3])[..., ::-1]  # BGR -> RGB\n",
    "\n",
    "# Resize image to 640x640\n",
    "image_resized, metadata = image_processor.preprocess_image(image_original)\n",
    "model_input = np.expand_dims(image_resized, 0)  # [1,3,640,640]\n",
    "[flat_predictions] = session.run(outputs, {inputs[0]: model_input})\n",
    "\n",
    "pred_bboxes = flat_predictions[:, 1:5]\n",
    "pred_scores = flat_predictions[:, 5]\n",
    "pred_joints = flat_predictions[:, 6:].reshape(len(pred_bboxes), -1, 3)\n",
    "\n",
    "p = PoseEstimationPrediction(\n",
    "    poses=pred_joints,\n",
    "    scores=pred_scores,\n",
    "    bboxes_xyxy=pred_bboxes,\n",
    "    edge_links=np.zeros((0,2)),\n",
    "    edge_colors=np.zeros((0,3)),\n",
    "    keypoint_colors=np.random.randint(0, 255, (pred_joints.shape[1], 3), dtype=np.uint8),\n",
    "    image_shape=(model_input.shape[2], model_input.shape[3]),\n",
    ")\n",
    "metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:11:58.487836400Z",
     "start_time": "2023-10-12T10:11:58.359718900Z"
    }
   },
   "id": "fb69db3d22a04e41"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "PoseEstimationPrediction(poses=array([[[ 9.33892746e+01,  3.71910980e+02,  2.40387648e-01],\n        [ 8.84057465e+01,  3.70263672e+02,  2.12954700e-01],\n        [ 9.35746536e+01,  3.70570801e+02,  2.05087841e-01],\n        ...,\n        [ 9.88854828e+01,  4.10569458e+02,  2.41989732e-01],\n        [ 8.94517212e+01,  4.17730042e+02,  1.46408051e-01],\n        [ 1.00782990e+02,  4.17586487e+02,  1.42147750e-01]],\n\n       [[ 1.18740906e+02,  3.46397156e+02,  4.15875465e-01],\n        [ 1.18329468e+02,  3.43987854e+02,  2.89114654e-01],\n        [ 1.17013855e+02,  3.44044250e+02,  4.05241132e-01],\n        ...,\n        [ 1.19035332e+02,  4.14153992e+02,  2.14025617e-01],\n        [ 1.07612869e+02,  4.25690491e+02,  1.13747567e-01],\n        [ 1.17378853e+02,  4.25458984e+02,  1.11210644e-01]],\n\n       [[ 4.15635132e+02,  5.14472900e+02,  2.96225727e-01],\n        [ 4.18026947e+02,  5.10613037e+02,  3.20348173e-01],\n        [ 4.17845551e+02,  5.10465118e+02,  1.34563357e-01],\n        ...,\n        [ 4.33143372e+02,  5.94633423e+02,  1.79093242e-01],\n        [ 4.14871338e+02,  6.24720093e+02,  1.60950810e-01],\n        [ 4.29955627e+02,  6.24496460e+02,  1.30747378e-01]],\n\n       ...,\n\n       [[ 6.31943893e+00,  3.19803284e+02,  2.33334541e-01],\n        [ 4.56894493e+00,  3.16665405e+02,  1.46367788e-01],\n        [ 5.90148020e+00,  3.16940796e+02,  2.34512687e-01],\n        ...,\n        [-1.96627617e-01,  3.57455536e+02,  2.33991444e-01],\n        [ 2.17940807e+00,  3.89801941e+02,  1.92468643e-01],\n        [-6.72112346e-01,  3.90139740e+02,  2.05798358e-01]],\n\n       [[ 6.25247154e+01,  3.19454865e+02,  4.68091160e-01],\n        [ 6.38847351e+01,  3.16827484e+02,  3.08891773e-01],\n        [ 5.94246864e+01,  3.16975342e+02,  4.79910582e-01],\n        ...,\n        [ 5.21140785e+01,  3.59242889e+02,  1.30137980e-01],\n        [ 6.29594574e+01,  3.67403870e+02,  9.61355567e-02],\n        [ 5.35664597e+01,  3.68449768e+02,  9.76549983e-02]],\n\n       [[ 6.09445305e+01,  2.56144314e+01,  6.63953900e-01],\n        [ 7.11565170e+01,  6.25597382e+00,  5.40939271e-01],\n        [ 4.14133072e+01,  7.08062935e+00,  6.08310282e-01],\n        ...,\n        [ 3.40217209e+01,  6.89691544e+01,  1.27843499e-01],\n        [ 7.28035126e+01,  1.43601318e+02,  9.21786129e-02],\n        [ 7.27402191e+01,  1.56649353e+02,  8.54525268e-02]]],\n      dtype=float32), scores=array([0.03933325, 0.03505552, 0.03478917, 0.03024065, 0.02591768,\n       0.0258736 , 0.02582949, 0.02570817, 0.02539107, 0.02532637,\n       0.02380559, 0.02378765, 0.02272761, 0.02252334, 0.02047765,\n       0.02019939, 0.02014679, 0.0199365 , 0.01971588, 0.01966649],\n      dtype=float32), bboxes_xyxy=array([[ 8.1108749e+01,  3.6276523e+02,  1.0630549e+02,  4.0783939e+02],\n       [ 9.3472572e+01,  3.3206680e+02,  1.3425014e+02,  4.0829224e+02],\n       [ 3.9273932e+02,  4.9263901e+02,  4.5511844e+02,  5.7135815e+02],\n       [ 1.5023174e+01,  2.8129398e+02,  5.1180389e+01,  3.3697476e+02],\n       [ 5.6432697e+02,  5.1840320e+02,  6.3217261e+02,  6.5255615e+02],\n       [ 5.3950250e+02,  3.7114255e+02,  5.7731787e+02,  4.9819284e+02],\n       [-3.1652374e+00, -6.4132576e+00,  1.4111818e+02,  9.7091354e+01],\n       [-2.0787670e+01, -5.7331018e+00,  1.0649762e+02,  7.0466354e+01],\n       [-3.2757523e+01, -7.1842690e+00,  1.1137049e+02,  1.4850111e+02],\n       [-4.5728931e+00,  1.9842491e+02,  4.1110657e+01,  2.7105209e+02],\n       [-2.8538303e+00, -6.6975800e+01,  9.0169113e+01,  2.5032312e+02],\n       [-8.0334702e+00, -2.6710453e+00,  3.3494434e+02,  3.0032532e+02],\n       [ 5.7719916e+02,  3.6678748e+02,  6.4350659e+02,  5.3759894e+02],\n       [-3.9946175e+00, -4.1155067e+01,  1.0497908e+02,  2.0189648e+02],\n       [ 1.2046942e+02,  3.4093637e+02,  1.4555463e+02,  3.9695294e+02],\n       [ 2.9441208e+02,  1.0311748e+02,  3.1204697e+02,  1.2857637e+02],\n       [ 5.3274115e+02,  1.0293113e+02,  5.7991742e+02,  2.4592476e+02],\n       [ 2.3719573e-01,  2.9826004e+02,  1.6348869e+01,  4.3033258e+02],\n       [ 4.2165382e+01,  3.0568152e+02,  7.1513069e+01,  3.4827820e+02],\n       [-2.6403000e+01, -5.0337410e+01,  1.0633637e+02,  7.5845764e+01]],\n      dtype=float32), edge_links=array([], shape=(0, 2), dtype=float64), edge_colors=array([], shape=(0, 3), dtype=float64), keypoint_colors=array([[157, 177,  40],\n       [245,   6, 170],\n       [ 27, 111,   7],\n       [  8, 189,  22],\n       [ 81,  95,  70],\n       [233,  81, 196],\n       [243,  67,  52],\n       [197, 101, 173],\n       [181,  77,  93],\n       [ 11,  18, 115],\n       [183,  21, 119],\n       [192,  24, 167],\n       [248,  49, 167],\n       [183, 225,  14],\n       [204, 159, 177],\n       [ 93,  36,  12],\n       [ 83,  90, 153]], dtype=uint8), image_shape=(640, 640))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:11:59.032510300Z",
     "start_time": "2023-10-12T10:11:59.003474700Z"
    }
   },
   "id": "a69bb6a4828c5959"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "PoseEstimationPrediction(poses=array([[[ 7.29603729e+01,  2.90555450e+02,  2.40387648e-01],\n        [ 6.90669861e+01,  2.89268494e+02,  2.12954700e-01],\n        [ 7.31052017e+01,  2.89508423e+02,  2.05087841e-01],\n        ...,\n        [ 7.72542801e+01,  3.20757385e+02,  2.41989732e-01],\n        [ 6.98841553e+01,  3.26351593e+02,  1.46408051e-01],\n        [ 7.87367096e+01,  3.26239441e+02,  1.42147750e-01]],\n\n       [[ 9.27663345e+01,  2.70622772e+02,  4.15875465e-01],\n        [ 9.24449005e+01,  2.68740509e+02,  2.89114654e-01],\n        [ 9.14170761e+01,  2.68784576e+02,  4.05241132e-01],\n        ...,\n        [ 9.29963531e+01,  3.23557800e+02,  2.14025617e-01],\n        [ 8.40725555e+01,  3.32570709e+02,  1.13747567e-01],\n        [ 9.17022324e+01,  3.32389832e+02,  1.11210644e-01]],\n\n       [[ 3.24714935e+02,  4.01931946e+02,  2.96225727e-01],\n        [ 3.26583557e+02,  3.98916443e+02,  3.20348173e-01],\n        [ 3.26441833e+02,  3.98800873e+02,  1.34563357e-01],\n        ...,\n        [ 3.38393250e+02,  4.64557373e+02,  1.79093242e-01],\n        [ 3.24118225e+02,  4.88062561e+02,  1.60950810e-01],\n        [ 3.35902832e+02,  4.87887848e+02,  1.30747378e-01]],\n\n       ...,\n\n       [[ 4.93706179e+00,  2.49846313e+02,  2.33334541e-01],\n        [ 3.56948829e+00,  2.47394852e+02,  1.46367788e-01],\n        [ 4.61053133e+00,  2.47610001e+02,  2.34512687e-01],\n        ...,\n        [-1.53615326e-01,  2.79262146e+02,  2.33991444e-01],\n        [ 1.70266259e+00,  3.04532776e+02,  1.92468643e-01],\n        [-5.25087774e-01,  3.04796661e+02,  2.05798358e-01]],\n\n       [[ 4.88474350e+01,  2.49574112e+02,  4.68091160e-01],\n        [ 4.99099503e+01,  2.47521469e+02,  3.08891773e-01],\n        [ 4.64255371e+01,  2.47636993e+02,  4.79910582e-01],\n        ...,\n        [ 4.07141228e+01,  2.80658508e+02,  1.30137980e-01],\n        [ 4.91870766e+01,  2.87034271e+02,  9.61355567e-02],\n        [ 4.18487968e+01,  2.87851379e+02,  9.76549983e-02]],\n\n       [[ 4.76129150e+01,  2.00112743e+01,  6.63953900e-01],\n        [ 5.55910301e+01,  4.88747978e+00,  5.40939271e-01],\n        [ 3.23541451e+01,  5.53174162e+00,  6.08310282e-01],\n        ...,\n        [ 2.65794697e+01,  5.38821526e+01,  1.27843499e-01],\n        [ 5.68777428e+01,  1.12188530e+02,  9.21786129e-02],\n        [ 5.68282967e+01,  1.22382309e+02,  8.54525268e-02]]],\n      dtype=float32), scores=array([0.03933325, 0.03505552, 0.03478917, 0.03024065, 0.02591768,\n       0.0258736 , 0.02582949, 0.02570817, 0.02539107, 0.02532637,\n       0.02380559, 0.02378765, 0.02272761, 0.02252334, 0.02047765,\n       0.02019939, 0.02014679, 0.0199365 , 0.01971588, 0.01966649],\n      dtype=float32), bboxes_xyxy=array([[ 6.33662109e+01,  2.83410339e+02,  8.30511627e+01,\n         3.18624512e+02],\n       [ 7.30254440e+01,  2.59427185e+02,  1.04882919e+02,\n         3.18978302e+02],\n       [ 3.06827606e+02,  3.84874237e+02,  3.55561279e+02,\n         4.46373566e+02],\n       [ 1.17368546e+01,  2.19760925e+02,  3.99846802e+01,\n         2.63261536e+02],\n       [ 4.40880432e+02,  4.05002502e+02,  4.93884857e+02,\n         5.09809509e+02],\n       [ 4.21486328e+02,  2.89955109e+02,  4.51029602e+02,\n         3.89213165e+02],\n       [-2.47284174e+00, -5.01035738e+00,  1.10248581e+02,\n         7.58526230e+01],\n       [-1.62403679e+01, -4.47898579e+00,  8.32012634e+01,\n         5.50518379e+01],\n       [-2.55918140e+01, -5.61271000e+00,  8.70081940e+01,\n         1.16016495e+02],\n       [-3.57257271e+00,  1.55019455e+02,  3.21176987e+01,\n         2.11759445e+02],\n       [-2.22955489e+00, -5.23248444e+01,  7.04446182e+01,\n         1.95564941e+02],\n       [-6.27614880e+00, -2.08675408e+00,  2.61675262e+02,\n         2.34629150e+02],\n       [ 4.50936829e+02,  2.86552704e+02,  5.02739532e+02,\n         4.19999176e+02],\n       [-3.12079477e+00, -3.21523972e+01,  8.20149078e+01,\n         1.57731628e+02],\n       [ 9.41167374e+01,  2.66356537e+02,  1.13714554e+02,\n         3.10119476e+02],\n       [ 2.30009430e+02,  8.05605316e+01,  2.43786697e+02,\n         1.00450287e+02],\n       [ 4.16204010e+02,  8.04149475e+01,  4.53060486e+02,\n         1.92128723e+02],\n       [ 1.85309172e-01,  2.33015656e+02,  1.27725544e+01,\n         3.36197327e+02],\n       [ 3.29417038e+01,  2.38813690e+02,  5.58695869e+01,\n         2.72092346e+02],\n       [-2.06273441e+01, -3.93261032e+01,  8.30752945e+01,\n         5.92545013e+01]], dtype=float32), edge_links=array([], shape=(0, 2), dtype=float64), edge_colors=array([], shape=(0, 3), dtype=float64), keypoint_colors=array([[157, 177,  40],\n       [245,   6, 170],\n       [ 27, 111,   7],\n       [  8, 189,  22],\n       [ 81,  95,  70],\n       [233,  81, 196],\n       [243,  67,  52],\n       [197, 101, 173],\n       [181,  77,  93],\n       [ 11,  18, 115],\n       [183,  21, 119],\n       [192,  24, 167],\n       [248,  49, 167],\n       [183, 225,  14],\n       [204, 159, 177],\n       [ 93,  36,  12],\n       [ 83,  90, 153]], dtype=uint8), image_shape=(640, 640))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = image_processor.postprocess_predictions(p, metadata)\n",
    "p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:11:59.389346300Z",
     "start_time": "2023-10-12T10:11:59.368336100Z"
    }
   },
   "id": "b156171ef5eb49ae"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xml><var name=\"_dummy_ipython_val\"  />\n",
      "<var name=\"_dummy_special_var\"  />\n",
      "<var name=\"COCO_DATA_DIR\" type=\"str\" qualifier=\"builtins\" value=\"g%3A/coco2017\" />\n",
      "<var name=\"ComposeProcessing\" type=\"ABCMeta\" qualifier=\"abc\" value=\"%3Cclass %27super_gradients.training.processing.processing.ComposeProcessing%27&gt;\" isContainer=\"True\" />\n",
      "<var name=\"DetectionOutputFormatMode\" type=\"EnumMeta\" qualifier=\"enum\" value=\"%3Cenum %27DetectionOutputFormatMode%27&gt;\" isContainer=\"True\" shape=\"2\" />\n",
      "<var name=\"ImagePermute\" type=\"ABCMeta\" qualifier=\"abc\" value=\"%3Cclass %27super_gradients.training.processing.processing.ImagePermute%27&gt;\" isContainer=\"True\" />\n",
      "<var name=\"KeypointsBottomRightPadding\" type=\"ABCMeta\" qualifier=\"abc\" value=\"%3Cclass %27super_gradients.training.processing.processing.KeypointsBottomRightPadding%27&gt;\" isContainer=\"True\" />\n",
      "<var name=\"KeypointsLongestMaxSizeRescale\" type=\"ABCMeta\" qualifier=\"abc\" value=\"%3Cclass %27super_gradients.training.processing.processing.KeypointsLongestMaxSizeRescale%27&gt;\" isContainer=\"True\" />\n",
      "<var name=\"PoseEstimationPrediction\" type=\"ABCMeta\" qualifier=\"abc\" value=\"%3Cclass %27super_gradients.training.utils.predict.predictions.PoseEstimationPrediction%27&gt;\" isContainer=\"True\" />\n",
      "<var name=\"example_input_image\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B%5B%5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   ...%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%5D%2C%2C  %5B%5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   ...%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%5D%2C%2C  %5B%5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   ...%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%2C   %5B0 0 0 ... 0 0 0%5D%5D%5D%5D\" isContainer=\"True\" shape=\"(1, 3, 640, 640)\" />\n",
      "<var name=\"flat_predictions\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B 0.00000000e%2B00  8.11087494e%2B01  3.62765228e%2B02 ...  1.00782990e%2B02%2C   4.17586487e%2B02  1.42147750e-01%5D%2C %5B 0.00000000e%2B00  9.34725723e%2B01  3.32066803e%2B02 ...  1.17378853e%2B02%2C   4.25458984e%2B02  1.11210644e-01%5D%2C %5B 0.00000000e%2B00  3.92739319e%2B02  4.92639008e%2B02 ...  4.29955627e%2B02%2C   6.24496460e%2B02  1.30747378e-01%5D%2C ...%2C %5B 0.00000000e%2B00  2.37195730e-01  2.98260040e%2B02 ... -6.72112346e-01%2C   3.90139740e%2B02  2.05798358e-01%5D%2C %5B 0.00000000e%2B00  4.21653824e%2B01  3.05681519e%2B02 ...  5.35664597e%2B01%2C   3.68449768e%2B02  9.76549983e-02%5D%2C %5B 0.00000000e%2B00 -2.64029999e%2B01 -5.03374100e%2B01 ...  7.27402191e%2B01%2C   1.56649353e%2B02  8.54525268e-02%5D%5D\" isContainer=\"True\" shape=\"(20, 57)\" />\n",
      "<var name=\"gt\" type=\"COCO\" qualifier=\"pycocotools.coco\" value=\"%3Cpycocotools.coco.COCO object at 0x00000255053F5600&gt;\" isContainer=\"True\" />\n",
      "<var name=\"gt_annotations_path\" type=\"str\" qualifier=\"builtins\" value=\"g%3A/coco2017%5Cannotations/person_keypoints_val2017.json\" />\n",
      "<var name=\"image_file\" type=\"str\" qualifier=\"builtins\" value=\"g%3A/coco2017%5Cimages/val2017%5C000000037988.jpg\" />\n",
      "<var name=\"image_files\" type=\"list\" qualifier=\"builtins\" value=\"%5B%27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000139.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000285.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000632.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000724.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000776.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000785.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000802.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000872.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000000885.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001000.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001268.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001296.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001353.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001425.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001490.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001503.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001532.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001584.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001675.jpg%27%2C %27g%3A/coco2017%5C%5Cimages/val2017%5C%5C000000001761.jpg%27%2C %27g%3A/coco2017%5C%5Cimage...\" isContainer=\"True\" shape=\"5000\" />\n",
      "<var name=\"image_original\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B%5B 47  53  67%5D%2C  %5B 33  39  51%5D%2C  %5B 37  46  55%5D%2C  ...%2C  %5B122 163 215%5D%2C  %5B114 157 208%5D%2C  %5B120 163 214%5D%5D%2C%2C %5B%5B 29  35  49%5D%2C  %5B 18  24  36%5D%2C  %5B 20  29  38%5D%2C  ...%2C  %5B113 156 207%5D%2C  %5B108 151 204%5D%2C  %5B111 157 209%5D%5D%2C%2C %5B%5B 31  34  49%5D%2C  %5B 20  26  38%5D%2C  %5B 23  32  41%5D%2C  ...%2C  %5B114 160 212%5D%2C  %5B111 157 209%5D%2C  %5B115 161 213%5D%5D%2C%2C ...%2C%2C %5B%5B252 255 255%5D%2C  %5B183 197 197%5D%2C  %5B  4  25  26%5D%2C  ...%2C  %5B145 160 181%5D%2C  %5B116 131 150%5D%2C  %5B 82  97 116%5D%5D%2C%2C %5B%5B165 179 188%5D%2C  %5B172 182 192%5D%2C  %5B 53  61  74%5D%2C  ...%2C  %5B108 120 136%5D%2C  %5B100 112 128%5D%2C  %5B109 121 137%5D%5D%2C%2C %5B%5B 50  57  73%5D%2C  %5B 49  58  73%5D%2C  %5B115 124 141%5D%2C  ...%2C  %5B108 117 132%5D%2C  %5B110 120 132%5D%2C  %5B114 122 135%5D%5D%5D\" isContainer=\"True\" shape=\"(500, 375, 3)\" />\n",
      "<var name=\"image_processor\" type=\"ComposeProcessing\" qualifier=\"super_gradients.training.processing.processing\" value=\"%3Csuper_gradients.training.processing.processing.ComposeProcessing object at 0x000002568BC168F0&gt;\" isContainer=\"True\" />\n",
      "<var name=\"image_resized\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B%5B 47  37  35 ... 127 127 127%5D%2C  %5B 35  27  24 ... 127 127 127%5D%2C  %5B 30  22  20 ... 127 127 127%5D%2C  ...%2C  %5B174 159 158 ... 127 127 127%5D%2C  %5B172 157 160 ... 127 127 127%5D%2C  %5B190 162 155 ... 127 127 127%5D%5D%2C%2C %5B%5B 53  44  42 ... 127 127 127%5D%2C  %5B 41  33  31 ... 127 127 127%5D%2C  %5B 34  28  27 ... 127 127 127%5D%2C  ...%2C  %5B174 158 156 ... 127 127 127%5D%2C  %5B171 155 156 ... 127 127 127%5D%2C  %5B189 160 150 ... 127 127 127%5D%5D%2C%2C %5B%5B 67  56  53 ... 127 127 127%5D%2C  %5B 55  45  42 ... 127 127 127%5D%2C  %5B 49  41  38 ... 127 127 127%5D%2C  ...%2C  %5B166 151 147 ... 127 127 127%5D%2C  %5B166 150 150 ... 127 127 127%5D%2C  %5B185 155 144 ... 127 127 127%5D%5D%5D\" isContainer=\"True\" shape=\"(3, 640, 640)\" />\n",
      "<var name=\"images_path\" type=\"str\" qualifier=\"builtins\" value=\"g%3A/coco2017%5Cimages/val2017\" />\n",
      "<var name=\"inputs\" type=\"list\" qualifier=\"builtins\" value=\"%5B%27onnx%3A%3ACast_0%27%5D\" isContainer=\"True\" shape=\"1\" />\n",
      "<var name=\"metadata\" type=\"ComposeProcessingMetadata\" qualifier=\"super_gradients.training.processing.processing\" value=\"ComposeProcessingMetadata%28metadata_lst=%5BRescaleMetadata%28original_shape=%28500%2C 375%29%2C scale_factor_h=1.28%2C scale_factor_w=1.28%29%2C DetectionPadToSizeMetadata%28padding_coordinates=PaddingCoordinates%28top=0%2C bottom=0%2C left=0%2C right=160%29%29%2C None%5D%29\" isContainer=\"True\" />\n",
      "<var name=\"model\" type=\"YoloNASPose_N\" qualifier=\"super_gradients.training.models.pose_estimation_models.yolo_nas_pose.yolo_nas_pose_variants\" value=\"YoloNASPose_N%28%0A  %28backbone%29%3A NStageBackbone%28%0A    %28stem%29%3A YoloNASStem%28%0A      %28conv%29%3A QARepVGGBlock%28%0A        %28nonlinearity%29%3A ReL...8%2C 34%2C kernel_size=%281%2C 1%29%2C stride=%281%2C 1%29%29%0A      %28cls_dropout_rate%29%3A Identity%28%29%0A      %28reg_dropout_rate%29%3A Identity%28%29%0A    %29%0A  %29%0A%29\" isContainer=\"True\" />\n",
      "<var name=\"model_input\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B%5B%5B 47  37  35 ... 127 127 127%5D%2C   %5B 35  27  24 ... 127 127 127%5D%2C   %5B 30  22  20 ... 127 127 127%5D%2C   ...%2C   %5B174 159 158 ... 127 127 127%5D%2C   %5B172 157 160 ... 127 127 127%5D%2C   %5B190 162 155 ... 127 127 127%5D%5D%2C%2C  %5B%5B 53  44  42 ... 127 127 127%5D%2C   %5B 41  33  31 ... 127 127 127%5D%2C   %5B 34  28  27 ... 127 127 127%5D%2C   ...%2C   %5B174 158 156 ... 127 127 127%5D%2C   %5B171 155 156 ... 127 127 127%5D%2C   %5B189 160 150 ... 127 127 127%5D%5D%2C%2C  %5B%5B 67  56  53 ... 127 127 127%5D%2C   %5B 55  45  42 ... 127 127 127%5D%2C   %5B 49  41  38 ... 127 127 127%5D%2C   ...%2C   %5B166 151 147 ... 127 127 127%5D%2C   %5B166 150 150 ... 127 127 127%5D%2C   %5B185 155 144 ... 127 127 127%5D%5D%5D%5D\" isContainer=\"True\" shape=\"(1, 3, 640, 640)\" />\n",
      "<var name=\"outputs\" type=\"list\" qualifier=\"builtins\" value=\"%5B%27graph2_flat_predictions%27%5D\" isContainer=\"True\" shape=\"1\" />\n",
      "<var name=\"p\" type=\"PoseEstimationPrediction\" qualifier=\"super_gradients.training.utils.predict.predictions\" value=\"PoseEstimationPrediction%28poses=array%28%5B%5B%5B 7.29603729e%2B01%2C  2.90555450e%2B02%2C  2.40387648e-01%5D%2C%0A        %5B 6.90669861e%2B01%2C  2.89268...%5B183%2C 225%2C  14%5D%2C%0A       %5B204%2C 159%2C 177%5D%2C%0A       %5B 93%2C  36%2C  12%5D%2C%0A       %5B 83%2C  90%2C 153%5D%5D%2C dtype=uint8%29%2C image_shape=%28640%2C 640%29%29\" isContainer=\"True\" shape=\"20\" />\n",
      "<var name=\"pred_bboxes\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B 8.1108749e%2B01  3.6276523e%2B02  1.0630549e%2B02  4.0783939e%2B02%5D%2C %5B 9.3472572e%2B01  3.3206680e%2B02  1.3425014e%2B02  4.0829224e%2B02%5D%2C %5B 3.9273932e%2B02  4.9263901e%2B02  4.5511844e%2B02  5.7135815e%2B02%5D%2C %5B 1.5023174e%2B01  2.8129398e%2B02  5.1180389e%2B01  3.3697476e%2B02%5D%2C %5B 5.6432697e%2B02  5.1840320e%2B02  6.3217261e%2B02  6.5255615e%2B02%5D%2C %5B 5.3950250e%2B02  3.7114255e%2B02  5.7731787e%2B02  4.9819284e%2B02%5D%2C %5B-3.1652374e%2B00 -6.4132576e%2B00  1.4111818e%2B02  9.7091354e%2B01%5D%2C %5B-2.0787670e%2B01 -5.7331018e%2B00  1.0649762e%2B02  7.0466354e%2B01%5D%2C %5B-3.2757523e%2B01 -7.1842690e%2B00  1.1137049e%2B02  1.4850111e%2B02%5D%2C %5B-4.5728931e%2B00  1.9842491e%2B02  4.1110657e%2B01  2.7105209e%2B02%5D%2C %5B-2.8538303e%2B00 -6.6975800e%2B01  9.0169113e%2B01  2.5032312e%2B02%5D%2C %5B-8.0334702e%2B00 -2.6710453e%2B00  3.3494434e%2B02  3.0032532e%2B02%5D%2C %5B 5.7719916e%2B02  3.6678748e%2B02  6.4350659e%2B02  5.3759894e%2B02%5D%2C %5B-3.9946175e%2B00 -4.1155067e%2B01  1.0497908e%2B02  2.0189648e%2B02%5D%2C %5B 1.2046942e%2B02  3.4093637e%2B02  1.4555463e%2B02  3.9695294e%2B02%5D%2C %5B 2.9441208e%2B02  1.0311748e%2B02  3.1204697e%2B02  1.28576...\" isContainer=\"True\" shape=\"(20, 4)\" />\n",
      "<var name=\"pred_joints\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B%5B%5B 9.33892746e%2B01  3.71910980e%2B02  2.40387648e-01%5D%2C  %5B 8.84057465e%2B01  3.70263672e%2B02  2.12954700e-01%5D%2C  %5B 9.35746536e%2B01  3.70570801e%2B02  2.05087841e-01%5D%2C  ...%2C  %5B 9.88854828e%2B01  4.10569458e%2B02  2.41989732e-01%5D%2C  %5B 8.94517212e%2B01  4.17730042e%2B02  1.46408051e-01%5D%2C  %5B 1.00782990e%2B02  4.17586487e%2B02  1.42147750e-01%5D%5D%2C%2C %5B%5B 1.18740906e%2B02  3.46397156e%2B02  4.15875465e-01%5D%2C  %5B 1.18329468e%2B02  3.43987854e%2B02  2.89114654e-01%5D%2C  %5B 1.17013855e%2B02  3.44044250e%2B02  4.05241132e-01%5D%2C  ...%2C  %5B 1.19035332e%2B02  4.14153992e%2B02  2.14025617e-01%5D%2C  %5B 1.07612869e%2B02  4.25690491e%2B02  1.13747567e-01%5D%2C  %5B 1.17378853e%2B02  4.25458984e%2B02  1.11210644e-01%5D%5D%2C%2C %5B%5B 4.15635132e%2B02  5.14472900e%2B02  2.96225727e-01%5D%2C  %5B 4.18026947e%2B02  5.10613037e%2B02  3.20348173e-01%5D%2C  %5B 4.17845551e%2B02  5.10465118e%2B02  1.34563357e-01%5D%2C  ...%2C  %5B 4.33143372e%2B02  5.94633423e%2B02  1.79093242e-01%5D%2C  %5B 4.14871338e%2B02  6.24720093e%2B02  1.60950810e-01%5D%2C  %5B 4.29955627e%2B02  6.24496460e%2B02  1.30747378e-01%5D%5D%2C%2C ...%2C%2C %5B%5B 6.31943893e%2B00  3.19803284e%2B02...\" isContainer=\"True\" shape=\"(20, 17, 3)\" />\n",
      "<var name=\"pred_scores\" type=\"ndarray\" qualifier=\"numpy\" value=\"%5B0.03933325 0.03505552 0.03478917 0.03024065 0.02591768 0.0258736%2C 0.02582949 0.02570817 0.02539107 0.02532637 0.02380559 0.02378765%2C 0.02272761 0.02252334 0.02047765 0.02019939 0.02014679 0.0199365%2C 0.01971588 0.01966649%5D\" isContainer=\"True\" shape=\"(20,)\" />\n",
      "<var name=\"predictions\" type=\"list\" qualifier=\"builtins\" value=\"%5BPoseEstimationPredictions%28poses=array%28%5B%5B%5B4.29200867e%2B02%2C 1.68755508e%2B02%2C 4.32034373e-01%5D%2C%0A        %5B4.30370575e%2B02%2C 1.65921539e...68%5D%2C%0A       %5B422.43637%2C 154.64363%2C 459.6042 %2C 278.86554%5D%2C%0A       %5B423.84497%2C 154.0466 %2C 458.9796 %2C 218.92961%5D%5D%2C dtype=float32%29%29%2C PoseEstimationPredictions%28poses=array%28%5B%5B%5B4.41130615e%2B02%2C 3.23886528e%2B01%2C 3.61385226e-01%5D%2C%0A        %5B4.40266937e%2B02%2C 3.00071526e...    %5B540.8367  %2C 425.77982 %2C 580.3495  %2C 541.9211  %5D%2C%0A       %5B272.1625  %2C 278.0738  %2C 326.21945 %2C 319.41156 %5D%5D%2C dtype=float32%29%29%2C PoseEstimationPredictions%28poses=array%28%5B%5B%5B 2.0123019e%2B02%2C  2.2610402e%2B00%2C  3.9632335e-01%5D%2C%0A        %5B 2.0223947e%2B02%2C  9.6478844e...         6.84454956e%2B01%5D%2C%0A       %5B 8.64221802e%2B01%2C -1.20923309e%2B01%2C  1.42000992e%2B02%2C%0A         7.60762329e%2B01%5D%5D%2C dtype=float32%29%29%2C PoseEstimationPredictions%28poses=array%28%5B%5B%5B 7.29603729e%2B01%2C  2.90555450e%2B02%2C  2.40387648e-01%5D%2C%0A        %5B 6.90669861e%2B01%2C  2.8926...         2.72092346e%2B02%5D%2C%0A       %5B-2.06273441e%2B01%2C -3.93261032e%2B01%2C  8.30752945e%2B01%2C%0A         5....\" isContainer=\"True\" shape=\"346\" />\n",
      "<var name=\"result\" type=\"PoseEstimationModelExportResult\" qualifier=\"super_gradients.module_interfaces.exportable_pose_estimation\" value=\"%0AModel exported successfully to yolo_nas_pose_n.onnx%0AModel expects input image of shape %5B1%2C 3%2C 640%2C 640%5D%0AInput image dtype is ...merate%28pred_joints%5Bi%5D%29%3A%22%29%0A        print%28f%22Joint %7B%7Bjoint_index%7D%7D has coordinates x=%7B%7Bx%7D%7D%2C y=%7B%7By%7D%7D%2C confidence=%7B%7Bconfidence%7D%7D%22%29%0A%0A\" isContainer=\"True\" />\n",
      "<var name=\"session\" type=\"InferenceSession\" qualifier=\"onnxruntime.capi.onnxruntime_inference_collection\" value=\"%3Connxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x000002557A681B10&gt;\" isContainer=\"True\" />\n",
      "</xml>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4999/5000 [07:33<00:00, 11.00it/s]"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for image_file in tqdm(image_files):\n",
    "    image_original = cv2.imread(image_file)[..., ::-1]  # BGR -> RGB\n",
    "\n",
    "    # Resize image to 640x640\n",
    "    image_resized, metadata = image_processor.preprocess_image(image_original)\n",
    "    model_input = np.expand_dims(image_resized, 0)  # [1,3,640,640]\n",
    "    [flat_predictions] = session.run(outputs, {inputs[0]: model_input})\n",
    "\n",
    "    pred_bboxes = flat_predictions[:, 1:5]\n",
    "    pred_scores = flat_predictions[:, 5]\n",
    "    pred_joints = flat_predictions[:, 6:].reshape(len(pred_bboxes), -1, 3)\n",
    "\n",
    "    p = PoseEstimationPrediction(\n",
    "        poses=pred_joints,\n",
    "        scores=pred_scores,\n",
    "        bboxes_xyxy=pred_bboxes,\n",
    "        edge_links=np.zeros((0,2)),\n",
    "        edge_colors=np.zeros((0,3)),\n",
    "        keypoint_colors=np.random.randint(0, 255, (pred_joints.shape[1], 3), dtype=np.uint8),\n",
    "        image_shape=(model_input.shape[2], model_input.shape[3]),\n",
    "    )\n",
    "    p = image_processor.postprocess_predictions(p, metadata)\n",
    "\n",
    "    predictions.append(PoseEstimationPredictions(\n",
    "        poses=p.poses,\n",
    "        scores=p.scores,\n",
    "        bboxes_xyxy=p.bboxes_xyxy,\n",
    "    ))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:19:33.639149900Z",
     "start_time": "2023-10-12T10:11:59.525479100Z"
    }
   },
   "id": "e939c4ad4ae9d3c6"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekhve\\.conda\\envs\\sg\\lib\\site-packages\\json_tricks\\encoders.py:394: UserWarning: json-tricks: numpy scalar serialization is experimental and may work differently in future versions\n",
      "  warnings.warn('json-tricks: numpy scalar serialization is experimental and may work differently in future versions')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=4.60s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *keypoints*\n",
      "DONE (t=17.84s).\n",
      "Accumulating evaluation results...\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json_tricks as json\n",
    "import collections\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def predictions_to_coco(predictions, image_files):\n",
    "    predicted_poses = []\n",
    "    predicted_scores = []\n",
    "    non_empty_image_ids = []\n",
    "    for image_file, image_predictions in zip(image_files, predictions):\n",
    "        non_empty_image_ids.append(int(os.path.splitext(os.path.basename(image_file))[0]))\n",
    "        predicted_poses.append(image_predictions.poses)\n",
    "        predicted_scores.append(image_predictions.scores)\n",
    "\n",
    "    coco_pred = _coco_convert_predictions_to_dict(predicted_poses, predicted_scores, non_empty_image_ids)\n",
    "    return coco_pred\n",
    "\n",
    "\n",
    "def _coco_process_keypoints(keypoints):\n",
    "    tmp = keypoints.copy()\n",
    "    if keypoints[:, 2].max() > 0:\n",
    "        num_keypoints = keypoints.shape[0]\n",
    "        for i in range(num_keypoints):\n",
    "            tmp[i][0:3] = [float(keypoints[i][0]), float(keypoints[i][1]), float(keypoints[i][2])]\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def _coco_convert_predictions_to_dict(predicted_poses, predicted_scores, image_ids):\n",
    "    kpts = collections.defaultdict(list)\n",
    "    for poses, scores, image_id_int in zip(predicted_poses, predicted_scores, image_ids):\n",
    "\n",
    "        for person_index, kpt in enumerate(poses):\n",
    "            area = (np.max(kpt[:, 0]) - np.min(kpt[:, 0])) * (np.max(kpt[:, 1]) - np.min(kpt[:, 1]))\n",
    "            kpt = _coco_process_keypoints(kpt)\n",
    "            kpts[image_id_int].append(\n",
    "                {\"keypoints\": kpt[:, 0:3], \"score\": float(scores[person_index]), \"image\": image_id_int, \"area\": area})\n",
    "\n",
    "    oks_nmsed_kpts = []\n",
    "    # image x person x (keypoints)\n",
    "    for img in kpts.keys():\n",
    "        # person x (keypoints)\n",
    "        img_kpts = kpts[img]\n",
    "        # person x (keypoints)\n",
    "        # do not use nms, keep all detections\n",
    "        keep = []\n",
    "        if len(keep) == 0:\n",
    "            oks_nmsed_kpts.append(img_kpts)\n",
    "        else:\n",
    "            oks_nmsed_kpts.append([img_kpts[_keep] for _keep in keep])\n",
    "\n",
    "    classes = [\"__background__\", \"person\"]\n",
    "    _class_to_coco_ind = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "    data_pack = [\n",
    "        {\"cat_id\": _class_to_coco_ind[cls], \"cls_ind\": cls_ind, \"cls\": cls, \"ann_type\": \"keypoints\",\n",
    "         \"keypoints\": oks_nmsed_kpts}\n",
    "        for cls_ind, cls in enumerate(classes)\n",
    "        if not cls == \"__background__\"\n",
    "    ]\n",
    "\n",
    "    results = _coco_keypoint_results_one_category_kernel(data_pack[0], num_joints=17)\n",
    "    return results\n",
    "\n",
    "\n",
    "def _coco_keypoint_results_one_category_kernel(data_pack, num_joints: int):\n",
    "    cat_id = data_pack[\"cat_id\"]\n",
    "    keypoints = data_pack[\"keypoints\"]\n",
    "    cat_results = []\n",
    "\n",
    "    for img_kpts in keypoints:\n",
    "        if len(img_kpts) == 0:\n",
    "            continue\n",
    "\n",
    "        _key_points = np.array([img_kpts[k][\"keypoints\"] for k in range(len(img_kpts))])\n",
    "        key_points = np.zeros((_key_points.shape[0], num_joints * 3), dtype=np.float32)\n",
    "\n",
    "        for ipt in range(num_joints):\n",
    "            key_points[:, ipt * 3 + 0] = _key_points[:, ipt, 0]\n",
    "            key_points[:, ipt * 3 + 1] = _key_points[:, ipt, 1]\n",
    "            # keypoints score.\n",
    "            key_points[:, ipt * 3 + 2] = _key_points[:, ipt, 2]\n",
    "\n",
    "        for k in range(len(img_kpts)):\n",
    "            kpt = key_points[k].reshape((num_joints, 3))\n",
    "            left_top = np.amin(kpt, axis=0)\n",
    "            right_bottom = np.amax(kpt, axis=0)\n",
    "\n",
    "            w = right_bottom[0] - left_top[0]\n",
    "            h = right_bottom[1] - left_top[1]\n",
    "\n",
    "            cat_results.append(\n",
    "                {\n",
    "                    \"image_id\": img_kpts[k][\"image\"],\n",
    "                    \"category_id\": cat_id,\n",
    "                    \"keypoints\": list(key_points[k]),\n",
    "                    \"score\": img_kpts[k][\"score\"],\n",
    "                    \"bbox\": list([left_top[0], left_top[1], w, h]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return cat_results\n",
    "\n",
    "\n",
    "coco_pred = predictions_to_coco(predictions, image_files)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    res_file = os.path.join(td, \"keypoints_coco2017_results.json\")\n",
    "\n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump(coco_pred, f)\n",
    "\n",
    "    coco_dt = copy.deepcopy(gt)\n",
    "    coco_dt = coco_dt.loadRes(res_file)\n",
    "\n",
    "    coco_evaluator = COCOeval(gt, coco_dt, iouType=\"keypoints\")\n",
    "    coco_evaluator.evaluate()  # run per image evaluation\n",
    "    coco_evaluator.accumulate()  # accumulate per image results\n",
    "    coco_evaluator.summarize()  # display summary metrics of results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:24:46.903401Z",
     "start_time": "2023-10-12T10:21:27.975226100Z"
    }
   },
   "id": "a077a3e5d1ed0342"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.59636437, 0.83149004, 0.65547537, 0.53969412, 0.68500305,\n       0.65593514, 0.87893577, 0.71095718, 0.5970773 , 0.73823857])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_evaluator.stats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:24:46.928080Z",
     "start_time": "2023-10-12T10:24:46.904400600Z"
    }
   },
   "id": "bc3ab706ff4eeb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7f1f02de114a3ed4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
